In this chapter, we begin by presenting the experimental datasets, CIFAR10 and CIFAR100 along with the image feature extraction process. Next we adapt three methods overviewed in Chapter \ref{bg} to reduce the size of image dataset, called the Patterns by Ordered Projections (POP) \cite{Riquelme2003a}, Enhanced Global Density-based Instance Selection (EGDIS) \cite{Malhat2020}, and Curriculum Learning (CL) \cite{Hacohen2019a}. Then we propose our edited data reduction method, called Weighted Curriculum Learning (WCL), based on CL scores and Boundary Based Curriculum Learning (BBCL), based on the EGDIS selected boundary instances. After that, our work is focused on the comprehensive evaluation of the methods. We illustrate the data reduction geometry patterns with three generated datasets, blobs, moons and circles. We then describe the model fitting procedure of the logistic regression algorithm. Finally, we extend the reduction pipeline to deep learning method with the particular network, DenseNet121. Figure \ref{Fig.reduction_pip} gives the pipeline overview of this project.  

 \begin{figure}[H]
 \centering
 \includegraphics[width=1\textwidth]{src/pipeline.png}
 \caption{Overview of the data reduction pipeline.}
 \label{Fig.reduction_pip}
 \end{figure}

\section{Datasets and Image Feature Extraction}
We choose to use CIFAR10 and CIFAR100 \cite{Krizhevsky2009} as our experimental datasets which contain 6,000 and 600 tiny images of size 32$\times$32 per class respectively. The advantage of CIFAR is that they are large in the number of images and small in the size of images. With CIFAR datasets, we could train the network faster thus explore the reduction rate for a wider scale within the required timetable. Another advantage is that CIFAR datasets can reflect the performance of data reduction algorithms for both simple dataset and hard dataset in term of classification accuracy. According to Kornblith et al. \cite{Kornblith2018}, the test set results indicate that CIFAR10 is very easy to classify and CIFAR100 is as difficult as other high resolution datasets such as the Describable Textures Dataset (DTD) \cite{Cimpoi2014}, Food-101 \cite{Bossard2014}. These features could gain us thorough and representative evaluation results with limited compute resources.

After scaling up the image size to 331 and transforming the images into range 0 to 1 by dividing 255, we therefore performed the feature extraction task. The goal was to provide structural data for the data reduction algorithms to work with. We did this job with pre-trained NasNetLarge \cite{Zoph2018} because Kornblith et al. \cite{Kornblith2018} have evaluated the quality of extracted features and the quality of extracted features is good enough. Their experiments show that the classification scores with simple logistic regression are very close to the state-of-art classification scores with CNN. In order to simplify the implementation, we chose to use the Keras implemented NasNetLarge network from TensorFlow Hub, which is designed to get feature vectors from images \cite{tensorhub_nasnet}. However, the original shape of the feature vector is 4032 and it would take longer time to run the reduction algorithms. To speed up the reduction process, we trained another network with two FC layers. The depth of the first FC layer is 128 and we took the outputs as compressed feature vectors. The test accuracy is also reported as the baseline performance. Figure \ref{Fig.compress_layer} represents the network structure. We also trained a batch normalisation layer after the 128-D FC layer to limit the feature range. This ensured that the Euclidean distance between two vectors wouldn't be dominated by dimensions with wider range.


 \begin{figure}[H]
 \centering
 \includegraphics[width=0.9\textwidth]{src/compress_layer.png}
 \caption{Network structure to compress the extracted feature vectors.}
 \label{Fig.compress_layer}
 \end{figure}

\section{Difficulty Tuneable Algorithms}
Before presenting the evaluation plans, it should be noted that the algorithms described in \ref{ibalgorithm} are not perfect. First of all, POP and EGDIS are not deep learning based algorithms so the CNN may not work well with selected samples. Also, although CL is target hypothesis based algorithm, by keeping easy samples only may limit the highest performance that the network could achieve. Therefore, our first contribution is to enhance CL with the ability to contain a tuneable proportion of relatively difficult samples. We proposed two variations:

\begin{enumerate}
\item Weighted Curriculum Learning, which select samples according to classification scores.
\item Boundary Enhanced Weighted Curriculum Learning, which selects a proportion of the EGDIS boundary samples first then fill in the subset with easier samples.
\end{enumerate}

We followed the requirement in \cite{Hacohen2019a} to select balanced subsets.

\subsection{Weighted Curriculum Learning}
Instead of selecting the top N samples after sorting the samples in ascending order based on classification scores, we normalised the scores as the survival probability. By dividing the sample score to the sum of all scores, the sum of all normalised scores will be 1 so that we can tread them as probability. In this way, sample with higher classification score would have higher probability to be selected. Therefore, not only the easy samples are selected, some hard examples are also selected. Because we only select the subset once, this behaviour should be able to achieve higher accuracy if the dataset is harder to classify and the network is powerful to learn from these hard examples. However, if the network is not capable of handling these hard examples, then the test accuracy may decay.




% WCL

\begin{algorithm}[H]
 \KwData{compressed 128-D feature vectors $M$}
  \KwIn{number of samples to select $m$, classification score for each sample $scores$, number of classes $n$}
 \KwOut{selected sample index by WCL}

$selected\_idx\_list = []$ \;

\ForEach{class label L}{
	scores = all sample scores with label L \;
	scores = scores / sum(scores) \;
	idx\_list = choose floor(m/n) samples based on scores \;
	selected\_idx\_list.append(idx\_list)) \;
}


\Return $selected\_idx\_list$ \;

\caption{WCL}
\end{algorithm}

\subsection{Boundary Based Weighted Curriculum Learning}
While WCL can reflect the difficulty of the datasets, it cannot guarantee to select enough hard examples for the network to mine the pattern. Therefore, we proposed the Boundary Based Weighted Curriculum Learning method to tune the amount of difficult samples in the selected subsets.

% BBWCL

\begin{algorithm}[H]
 \KwData{compressed 128-D feature vectors $M$}
  \KwIn{number of samples to select $m$, classification score for each sample $scores$, number of classes $n$, EGDIS boundary sample index list $egdis\_boundary\_index$, percent of boundary to select $p$}
 \KwOut{selected sample index by BWCL}

$selected\_idx\_list = []$ \;

\ForEach{class label L}{
	scores = all non-EGDIS sample scores with label L \;
	egdis\_boundary\_index\_L = all EGDIS boundary sample index with label L \;
	egdis\_idx = choose floor(m/n $\times$ p) samples from egdis\_boundary\_index\_L \;
	selected\_idx\_list.append(egdis\_idx)) \;
	
	scores = scores / sum(scores) \;
	idx\_list = choose floor(m/n$\times$ (1-p))
	 samples based on scores \;
	selected\_idx\_list.append(idx\_list)) \;
}


\Return $selected\_idx\_list$ \;

\caption{BWCL}
\end{algorithm}


\section{Evaluation Designs}
We have the following few experiments to evaluate the performance of the chosen three and the two proposed algorithms:

\begin{enumerate}
\item We extracted the feature vectors for CIFAR10 and CIFAR100. We visualised the extracted features with t-SNE.
\item We use CIFAR10 to explore the intrinsic behaviour of these methods. We reported the extraction time for both CIFAR10 and CIFAR100.
\item We use logistic regression to test all five methods. We also use the experiment results to decide how to select the CIFAR subsets.
\item We used these four datasets to evaluate them CNN, DenseNet121.
\end{enumerate}

