With the development of Convolutional Neural Networks (CNNs), many computer vision challenges have proven to achieve high performances on image datasets. Krizhevsky et al. \cite{Krizhevsky2017} designed an eight-layer CNN and outperformed the record in the ImageNet Large Scale Visual Recognition Competition (ILSVRC2012) by about 9.4 per cent. Three years later, the very deep CNN by He et al. \cite{He, He2016} surpassed human-level performance on the same dataset. It is possible to train CNNs with hundreds of layers due to the increment of dataset size, robust initialisation methods, development of GPU training frameworks, advanced regularisation skills, and network architectures such as highway connection \cite{He2016, Huang2017}. These achievements made deep CNNs the dominant choice for the image classification task. While deep architectures achieved high accuracies, the large-scale datasets also caused CNNs both time-consuming and eager for computing and storage resources. 

In order to overcome these drawbacks, recent research developed many approaches to accelerate the CNN convergence speed. These approaches typically fall into one of the two categories: select mini-batch samples non-uniformly \cite{Shrivastava2016, Li2017, Katharopoulos2017, Chang2017} or rank the order by which samples are fed into CNNs during training \cite{Bengio2009, Hacohen2019a}. Both methods need to evaluate sample classification scores. We have two options on when to evaluate the classification scores: at each training step or calculate once before the training process starts. We use the term \textbf{Current Hypothesis Method} and \textbf{Target Hypothesis Method} to refer to these situations \cite{Hacohen2019a}. However, we still need to train the whole training set or evaluate them during training. 

There are methods designed to reduce the number of training samples which have structured features called \textbf{instance selection} algorithms \cite{Olvera-Lopez2010, Aha1991, Brighton2002, Riquelme2003a}. The typical approach is to select samples that can maintain the decision boundary of machine learning algorithms such as k-nearest neighbour \cite{Malhat2020}. However, to the best of our knowledge, researchers have not developed an efficient pipeline to make them work with image datasets and CNNs \cite{Sun2014, Albelwi2016}. One reason is that images are not structural data. The other reason is that these selection algorithms are not optimised for CNNs. Hence, we need to build a pipeline for these algorithms to be compatible with CNNs.

The aim of this project is to establish an efficient pipeline to reduce the number of training samples needed for CNNs. To do so, we will adapt typical instance selection algorithms and the target hypothesis methods to reduce the size of training sets for CNNs. Before CNNs became popular, researchers tend to reduce the image dimensionality by transforming the images into feature vectors then classify these features with machine learning algorithms such as SVM \cite{Popovici2003, Csurka2004}. Since these extracted features can be considered as structural features, and the pre-calculated classification scores can reflect the sample importances for CNNs, it is possible to achieve a better reduction performance, in terms of classification accuracy and algorithm compute time. 

In specific, we will extract the image feature vectors as a pre-processing step. The existing instance selection algorithms are extended with the awareness of sample classification scores. Since there is no enough experience to guide us configure the algorithms for CNNs, we first explore the behaviours by visualising the selected samples and train the subsets with the logistic regression method. After tuning the hyper-parameters and selecting the same amount of subsets on the feature vectors, we train CNNs from scratch with corresponding images and report the relative accuracies. We then dive into the most suitable algorithm and build the trade-off framework, which can guide researchers to balance the relative accuracy and the number of samples selected. 

This dissertation is structured as follows: Chapter 2 describes the necessary background to understand image feature extraction, CNN dataset arrangement techniques, and the instance selection algorithms. We then cover the experiments to evaluate instance selection algorithms in Chapter 3. We explain the datasets and feature extraction procedures used in the experiments, detail the instance selection algorithms proposed for CNNs, and clarify the evaluation designs. The results of our experiments are reported in Chapter 4. In Chapter 5, we extend the adapted instance selection algorithms to select subsets for a given relative accuracy. Finally, in Chapter 6, we finish this thesis with a conclusion and a discussion about future work.



