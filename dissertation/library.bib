Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@inproceedings{Bossard2014,
abstract = {In this paper we address the problem of automatically recognizing pictured dishes. To this end, we introduce a novel method to mine discriminative parts using Random Forests (rf), which allows us to mine for parts simultaneously for all classes and to share knowledge among them. To improve efficiency of mining and classification, we only consider patches that are aligned with image superpixels, which we call components. To measure the performance of our rf component mining for food recognition, we introduce a novel and challenging dataset of 101 food categories, with 101'000 images. With an average accuracy of 50.76{\%}, our model outperforms alternative classification methods except for cnn, including svm classification on Improved Fisher Vectors and existing discriminative part-mining algorithms by 11.88{\%} and 8.13{\%}, respectively. On the challenging mit-Indoor dataset, our method compares nicely to other s-o-a component-based classification methods. {\textcopyright} 2014 Springer International Publishing.},
author = {Bossard, Lukas and Guillaumin, Matthieu and {Van Gool}, Luc},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-10599-4_29},
isbn = {9783319105987},
issn = {16113349},
keywords = {Discriminative part mining,Food recognition,Image classification,Random Forest},
number = {PART 6},
pages = {446--461},
publisher = {Springer Verlag},
title = {{Food-101 - Mining discriminative components with random forests}},
url = {https://link.springer.com/chapter/10.1007/978-3-319-10599-4{\_}29},
volume = {8694 LNCS},
year = {2014}
}
@article{Gabriel2004,
abstract = {A large number of MPI implementations are currently available, each of which emphasize different aspects of high-performance computing or are intended to solve a specific research problem. The result is a myriad of incompatible MPI implementations, all of which require separate installation, and the combination of which present significant logistical challenges for end users. Building upon prior research, and influenced by experience gained from the code bases of the LAM/MPI, LA-MPI, and FT-MPI projects, Open MPI is an all-new, production-quality MPI-2 implementation that is fundamentally centered around component concepts. Open MPI provides a unique combination of novel features previously unavailable in an open-source, production-quality implementation of MPI. Its component architecture provides both a stable platform for third-party research as well as enabling the run-time composition of independent software add-ons. This paper presents a high-level overview the goals, design, and implementation of Open MPI. {\textcopyright} Springer-Verlag 2004.},
author = {Gabriel, Edgar and Fagg, Graham E and Bosilca, George and Angskun, Thara and Dongarra, Jack J and Squyres, Jeffrey M and Sahay, Vishal and Kambadur, Prabhanjan and Barrett, Brian and Lumsdaine, Andrew and Castain, Ralph H and Daniel, David J and Graham, Richard L and Woodall, Timothy S},
doi = {10.1007/978-3-540-30218-6_19},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Gabriel et al. - Unknown - Open MPI Goals, Concept, and Design of a Next Generation MPI Implementation.pdf:pdf},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {97--104},
title = {{Open MPI: Goals, concept, and design of a next generation MPI implementation}},
volume = {3241},
year = {2004}
}
@article{Tatarchenko2019,
abstract = {Convolutional networks for single-view object reconstruction have shown impressive performance and have become a popular subject of research. All existing techniques are united by the idea of having an encoder-decoder network that performs non-trivial reasoning about the 3D structure of the output space. In this work, we set up two alternative approaches that perform image classification and retrieval respectively. These simple baselines yield better results than state-of-the-art methods, both qualitatively and quantitatively. We show that encoder-decoder methods are statistically indistinguishable from these baselines, thus indicating that the current state of the art in single-view object reconstruction does not actually perform reconstruction but image classification. We identify aspects of popular experimental procedures that elicit this behavior and discuss ways to improve the current state of research.},
archivePrefix = {arXiv},
arxivId = {1905.03678},
author = {Tatarchenko, Maxim and Richter, Stephan R. and Ranftl, Ren{\'{e}} and Li, Zhuwen and Koltun, Vladlen and Brox, Thomas},
eprint = {1905.03678},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Tatarchenko et al. - 2019 - What Do Single-view 3D Reconstruction Networks Learn.pdf:pdf},
month = {may},
title = {{What Do Single-view 3D Reconstruction Networks Learn?}},
url = {http://arxiv.org/abs/1905.03678},
year = {2019}
}
@inproceedings{Istrate2017,
abstract = {We propose an incremental training method that partitions the original network into sub-networks, which are then gradually incorporated in the running network during the training process. To allow for a smooth dynamic growth of the network, we introduce a look-ahead initialization that outperforms the random initialization. We demonstrate that our incremental approach reaches the reference network baseline ac- curacy. Additionally, it allows to identify smaller partitions of the original state-of-the-art network, that deliver the same final accuracy, by using only a fraction of the global number of parameters. This allows for a potential speedup of the training time of several factors. We report training results on CIFAR-10 for ResNet and VGGNet.},
archivePrefix = {arXiv},
arxivId = {1803.10232},
author = {Istrate, Roxana and Malossi, A. C.I. and Bekas, Costas and Nikolopoulos, Dimitrios},
booktitle = {CEUR Workshop Proceedings},
eprint = {1803.10232},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Istrate et al. - 2017 - Incremental training of deep convolutional neural networks.pdf:pdf},
issn = {16130073},
keywords = {CNNs,Look-ahead,Training algorithm},
month = {mar},
publisher = {CEUR-WS},
title = {{Incremental training of deep convolutional neural networks}},
url = {http://arxiv.org/abs/1803.10232},
volume = {1998},
year = {2017}
}
@techreport{Miyato,
abstract = {We propose a novel, projection based way to incorporate the conditional information into the discriminator of GANs that respects the role of the conditional information in the underlining probabilistic model. This approach is in contrast with most frameworks of conditional GANs used in application today, which use the conditional information by concatenating the (embedded) conditional vector to the feature vectors. With this modification, we were able to significantly improve the quality of the class conditional image generation on ILSVRC2012 (Im-ageNet) 1000-class image dataset from the current state-of-the-art result, and we achieved this with a single pair of a discriminator and a generator. We were also able to extend the application to super-resolution and succeeded in producing highly discriminative super-resolution images. This new structure also enabled high quality category transformation based on parametric functional transformation of conditional batch normalization layers in the generator. The code with Chainer (Tokui et al., 2015), generated images and pretrained models are available at https://github.com/pfnet-research/sngan{\_}projection.},
archivePrefix = {arXiv},
arxivId = {1802.05637v2},
author = {Miyato, Takeru and Koyama, Masanori},
eprint = {1802.05637v2},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Miyato, Koyama - Unknown - CGANS WITH PROJECTION DISCRIMINATOR.pdf:pdf},
title = {{CGANS WITH PROJECTION DISCRIMINATOR}},
url = {https://github.com/pfnet-research/sngan{\_}projection.}
}
@article{Fulgeri2019,
abstract = {When you see a person in a crowd, occluded by other persons, you miss visual information that can be used to recognize, re-identify or simply classify him or her. You can imagine its appearance given your experience, nothing more. Similarly, AI solutions can try to hallucinate missing information with specific deep learning architectures, suitably trained with people with and without occlusions. The goal of this work is to generate a complete image of a person, given an occluded version in input, that should be a) without occlusion b) similar at pixel level to a completely visible people shape c) capable to conserve similar visual attributes (e.g. male/female) of the original one. For the purpose, we propose a new approach by integrating the state-of-the-art of neural network architectures, namely U-nets and GANs, as well as discriminative attribute classification nets, with an architecture specifically designed to de-occlude people shapes. The network is trained to optimize a Loss function which could take into account the aforementioned objectives. As well we propose two datasets for testing our solution: the first one, occluded RAP, created automatically by occluding real shapes of the RAP dataset created by Li et al. (2016) (which collects also attributes of the people aspect); the second is a large synthetic dataset, AiC, generated in computer graphics with data extracted from the GTA video game, that contains 3D data of occluded objects by construction. Results are impressive and outperform any other previous proposal. This result could be an initial step to many further researches to recognize people and their behavior in an open crowded world.},
author = {Fulgeri, Federico and Fabbri, Matteo and Alletto, Stefano and Calderara, Simone and Cucchiara, Rita},
doi = {10.1016/j.cviu.2019.03.007},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Fulgeri et al. - 2019 - Can adversarial networks hallucinate occluded people with a plausible aspect.pdf:pdf},
issn = {1090235X},
journal = {Computer Vision and Image Understanding},
keywords = {Attribute recognition,GAN,Occlusions},
number = {January},
pages = {71--80},
publisher = {Elsevier Inc.},
title = {{Can adversarial networks hallucinate occluded people with a plausible aspect?}},
url = {https://doi.org/10.1016/j.cviu.2019.03.007},
volume = {182},
year = {2019}
}
@inproceedings{Ulusoy2015,
abstract = {This paper presents a novel probabilistic foundation for volumetric 3D reconstruction. We formulate the problem as inference in a Markov random field, which accurately captures the dependencies between the occupancy and appearance of each voxel, given all input images. Our main contribution is an approximate highly parallelized discrete-continuous inference algorithm to compute the marginal distributions of each voxel's occupancy and appearance. In contrast to the MAP solution, marginals encode the underlying uncertainty and ambiguity in the reconstruction. Moreover, the proposed algorithm allows for a Bayes optimal prediction with respect to a natural reconstruction loss. We compare our method to two state-of-the-art volumetric reconstruction algorithms on three challenging aerial datasets with LIDAR ground truth. Our experiments demonstrate that the proposed algorithm compares favorably in terms of reconstruction accuracy and the ability to expose reconstruction uncertainty.},
author = {Ulusoy, Ali Osman and Geiger, Andreas and Black, Michael J.},
booktitle = {Proceedings - 2015 International Conference on 3D Vision, 3DV 2015},
doi = {10.1109/3DV.2015.9},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Towards Probabilistic Volumetric Reconstruction using Ray Potentials.pdf:pdf},
isbn = {9781467383325},
keywords = {markov random field,multi-view stereo,ray potential,volumetric reconstruction},
month = {oct},
pages = {10--18},
publisher = {IEEE},
title = {{Towards Probabilistic Volumetric Reconstruction Using Ray Potentials}},
url = {http://ieeexplore.ieee.org/document/7335464/},
year = {2015}
}
@inproceedings{Li2015,
abstract = {Sparse linear models approximate target variable(s) by a sparse linear combination of input variables. The sparseness is realized through a regularization term. Since they are simple, fast, and able to select features, they are widely used in classification and regression. Essentially linear models are shallow feed-forward neural networks which have three limitations: (1) incompatibility to model non-linearity of features, (2) inability to learn high-level features, and (3) unnatural extensions to select features in multi-class case. Deep neural networks are models structured by multiple hidden layers with non-linear activation functions. Compared with linear models, they have two distinctive strengths: the capability to (1) model complex systems with non-linear structures, (2) learn high-level representation of features. Deep learning has been applied in many large and complex systems where deep models significantly outperform shallow ones. However, feature selection at the input level, which is very helpful to understand the nature of a complex system, is still not well-studied. In genome research, the cis-regulatory elements in noncoding DNA sequences play a key role in the expression of genes. Since the activity of regulatory elements involves highly interactive factors, a deep tool is strongly needed to discover informative features. In order to address the above limitations of shallow and deep models for selecting features of a complex system, we propose a deep feature selection model that (1) takes advantages of deep structures to model non-linearity and (2) conveniently selects a subset of features right at the input level for multi-class data.We applied this model to the identification of active enhancers and promoters by integrating multiple sources of genomic information. Results show that our model outperforms elastic net in terms of size of discriminative feature subset and classification accuracy.},
author = {Li, Yifeng and Chen, Chih Yu and Wasserman, Wyeth W.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-16706-0_20},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Li, Chen, Wasserman - 2015 - Deep feature selection Theory and application to identify enhancers and promoters.pdf:pdf},
isbn = {9783319167053},
issn = {16113349},
keywords = {Deep learning,Enhancer,Feature selection,Promoter},
pages = {205--217},
publisher = {Springer Verlag},
title = {{Deep feature selection: Theory and application to identify enhancers and promoters}},
volume = {9029},
year = {2015}
}
@inproceedings{Ranjan2018,
abstract = {Learned 3D representations of human faces are useful for computer vision problems such as 3D face tracking and reconstruction from images, as well as graphics applications such as character generation and animation. Traditional models learn a latent representation of a face using linear subspaces or higher-order tensor generalizations. Due to this linearity, they can not capture extreme deformations and non- linear expressions. To address this, we introduce a versatile model that learns a non-linear representation of a face using spectral convolutions on a mesh surface. We introduce mesh sampling operations that enable a hierarchical mesh representation that captures non-linear variations in shape and expression at multiple scales within the model. In a variational setting, our model samples diverse realistic 3D faces from a multivariate Gaussian distribution. Our training data consists of 20,466 meshes of extreme expressions captured over 12 different subjects. Despite limited training data, our trained model outperforms state-of-the-art face models with 50{\%} lower reconstruction error, while using 75{\%} fewer parameters. We show that, replacing the expression space of an existing state-of-the- art face model with our model, achieves a lower reconstruction error. Our data, model and code are available at http://coma.is.tue.mpg.de/.},
author = {Ranjan, Anurag and Bolkart, Timo and Sanyal, Soubhik and Black, Michael J.},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Ranjan et al. - 2018 - Generating 3D Faces using Convolutional Mesh Autoencoders.pdf:pdf},
pages = {704--720},
title = {{Generating 3D Faces using Convolutional Mesh Autoencoders}},
url = {http://openaccess.thecvf.com/content{\_}ECCV{\_}2018/html/Anurag{\_}Ranjan{\_}Generating{\_}3D{\_}Faces{\_}ECCV{\_}2018{\_}paper.html},
year = {2018}
}
@inproceedings{Fei-Fei2005,
abstract = {We propose a novel approach to learn and recognize natural scene categories. Unlike previous work [9, 17], it does not require experts to annotate the training set. We represent the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning. Each region is represented as part of a "theme". In previous work, such themes were learnt from hand-annotations of experts, while our method learns the theme distributions as well as the codewords distribution over the themes without supervision. We report satisfactory categorization performances on a large set of 13 categories of complex scenes.},
author = {Fei-Fei, Li and Perona, Pietro},
booktitle = {Proceedings - 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, CVPR 2005},
doi = {10.1109/CVPR.2005.16},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Fei-Fei, Perona - 2005 - A bayesian hierarchical model for learning natural scene categories.pdf:pdf},
isbn = {0769523722},
pages = {524--531},
title = {{A bayesian hierarchical model for learning natural scene categories}},
volume = {II},
year = {2005}
}
@misc{Olvera-Lopez2010,
abstract = {In supervised learning, a training set providing previously known information is used to classify new instances. Commonly, several instances are stored in the training set but some of them are not useful for classifying therefore it is possible to get acceptable classification rates ignoring non useful cases; this process is known as instance selection. Through instance selection the training set is reduced which allows reducing runtimes in the classification and/or training stages of classifiers. This work is focused on presenting a survey of the main instance selection methods reported in the literature. {\textcopyright} Springer Science+Business Media B.V. 2010.},
author = {Olvera-L{\'{o}}pez, J Arturo and Carrasco-Ochoa, J. Ariel and Mart{\'{i}}nez-Trinidad, J. Francisco and Kittler, Josef},
booktitle = {Artificial Intelligence Review},
doi = {10.1007/s10462-010-9165-y},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Olvera-L{\'{o}}pez et al. - 2010 - A review of instance selection methods.pdf:pdf},
issn = {02692821},
keywords = {Data reduction,Instance selection,Pre-processing,Supervised learning},
number = {2},
pages = {133--143},
publisher = {Springer Netherlands},
title = {{A review of instance selection methods}},
url = {https://link.springer.com/article/10.1007/s10462-010-9165-y},
volume = {34},
year = {2010}
}
@article{Gargantini1982,
abstract = {A new, effective way of storing octtrees for three-dimensional representation of objects is given. The 10 fields normally required to identify a node of an octtree are reduced to only one. Algorithms are presented for (i) mapping cubic pixels from and to space array (with subscripts I, J, K), (ii) finding the stereographic projections on the IJ, IK, and JK planes, (iii) performing union (intersection) of two objects centered on the same array, and (iv) finding the pixel adjacent to a given one in a specified direction. The newly proposed data structure is a (dynamically built) array of sorted octal codes which reflects the successive octant subdivisions; it represents a dramatic improvement with respect to octtrees when space complexity is considered. Also, the formulation of the procedures mentioned above takes advantage of this "natural" structure and results in very simple algorithms, easy to code and optimize. Some of the proposed procedures could also be implemented in parallel mode. {\textcopyright} 1982 Academic Press, Inc.},
author = {Gargantini, Irene},
doi = {10.1016/0146-664X(82)90058-2},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Gargantini - 1982 - Linear octtrees for fast processing of three-dimensional objects.pdf:pdf},
issn = {0146664X},
journal = {Computer Graphics and Image Processing},
month = {dec},
number = {4},
pages = {365--374},
publisher = {Academic Press},
title = {{Linear octtrees for fast processing of three-dimensional objects}},
url = {https://www.sciencedirect.com/science/article/pii/0146664X82900582},
volume = {20},
year = {1982}
}
@inproceedings{Park2019a,
abstract = {The rising volume of datasets has made training machine learning (ML) models a major computational cost in the enterprise. Given the iterative nature of model and parameter tuning, many analysts use a small sample of their entire data during their initial stage of analysis to make quick decisions (e.g., what features or hyperparameters to use) and use the entire dataset only in later stages (i.e., when they have converged to a specific model). This sampling, however, is performed in an ad-hoc fashion. Most practitioners cannot precisely capture the effect of sampling on the quality of their model, and eventually on their decision-making process during the tuning phase. Moreover, without systematic support for sampling operators, many optimizations and reuse opportunities are lost. In this paper, we introduce BlinkML, a system for fast, quality-guaranteed ML training. BlinkML allows users to make error-computation tradeoffs: instead of training a model on their full data (i.e., full model), BlinkML can quickly train an approximate model with quality guarantees using a sample. The quality guarantees ensure that, with high probability, the approximate model makes the same predictions as the full model. BlinkML currently supports any ML model that relies on maximum likelihood estimation (MLE), which includes Generalized Linear Models (e.g., linear regression, logistic regression, max entropy classifier, Poisson regression) as well as PPCA (Probabilistic Principal Component Analysis). Our experiments show that BlinkML can speed up the training of large-scale ML tasks by 6.26×-629× while guaranteeing the same predictions, with 95{\%} probability, as the full model.},
address = {New York, New York, USA},
archivePrefix = {arXiv},
arxivId = {1812.10564},
author = {Park, Yongjoo and Qing, Jingyi and Shen, Xiaoyang and Mozafari, Barzan},
booktitle = {Proceedings of the ACM SIGMOD International Conference on Management of Data},
doi = {10.1145/3299869.3300077},
eprint = {1812.10564},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Park et al. - 2019 - BlinkML Efficient maximum likelihood estimation with probabilistic guarantees(2).pdf:pdf},
isbn = {9781450356435},
issn = {07308078},
month = {jun},
pages = {1135--1152},
publisher = {Association for Computing Machinery},
title = {{BlinkML: Efficient maximum likelihood estimation with probabilistic guarantees}},
url = {http://dl.acm.org/citation.cfm?doid=3299869.3300077},
year = {2019}
}
@inproceedings{Cimpoi2014,
abstract = {Patterns and textures are key characteristics of many natural objects: a shirt can be striped, the wings of a butterfly can be veined, and the skin of an animal can be scaly. Aiming at supporting this dimension in image understanding, we address the problem of describing textures with semantic attributes. We identify a vocabulary of forty-seven texture terms and use them to describe a large dataset of patterns collected 'in the wild'. The resulting Describable Textures Dataset (DTD) is a basis to seek the best representation for recognizing describable texture attributes in images. We port from object recognition to texture recognition the Improved Fisher Vector (IFV) and Deep Convolutional-network Activation Features (DeCAF), and show that surprisingly, they both outperform specialized texture descriptors not only on our problem, but also in established material recognition datasets. We also show that our describable attributes are excellent texture descriptors, transferring between datasets and tasks, in particular, combined with IFV and DeCAF, they significantly outperform the state-of-the-art by more than 10{\%} on both FMD and KTH-TIPS-2b benchmarks. We also demonstrate that they produce intuitive descriptions of materials and Internet images.},
archivePrefix = {arXiv},
arxivId = {1311.3618},
author = {Cimpoi, Mircea and Maji, Subhransu and Kokkinos, Iasonas and Mohamed, Sammy and Vedaldi, Andrea},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.461},
eprint = {1311.3618},
isbn = {9781479951178},
issn = {10636919},
keywords = {Fisher Vector,attribute,convolutional neural network,material,recognition,texture},
month = {sep},
pages = {3606--3613},
publisher = {IEEE Computer Society},
title = {{Describing textures in the wild}},
url = {http://arxiv.org/abs/1311.3618},
year = {2014}
}
@inproceedings{Hacohen2019a,
abstract = {Training neural networks is traditionally done by providing a sequence of random mini-batches sampled uniformly from the entire training data. In this work, we analyze the effect of curriculum learning, which involves the non-uniform sampling of mini-batches, on the training of deep networks, and specifically CNNs trained for image recognition. To employ curriculum learning, the training algorithm must resolve 2 problems: (i) sort the training examples by difficulty; (ii) compute a series of mini-batches that exhibit an increasing level of difficulty. We address challenge (i) using two methods: transfer learning from some competitive "teacher" network, and bootstrapping. In our empirical evaluation, both methods show similar benefits in terms of increased learning speed and improved final performance on test data. We address challenge (ii) by investigating different pacing functions to guide the sampling. The empirical investigation includes a variety of network architectures, using images from CIFAR-10, CIFAR-100 and subsets of ImageNet. We conclude with a novel theoretical analysis of curriculum learning, where we show how it effectively modifies the optimization landscape. We then define the concept of an ideal curriculum, and show that under mild conditions it does not change the corresponding global minimum of the optimization function.},
archivePrefix = {arXiv},
arxivId = {1904.03626v3},
author = {Hacohen, Guy and Weinshall, Daphna},
booktitle = {36th International Conference on Machine Learning, ICML 2019},
eprint = {1904.03626v3},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Hacohen, Weinshall - Unknown - On The Power of Curriculum Learning in Training Deep Networks.pdf:pdf},
isbn = {9781510886988},
pages = {4483--4496},
title = {{On the power of curriculum learning in training deep networks}},
volume = {2019-June},
year = {2019}
}
@article{Weiss2016,
abstract = {Machine learning and data mining techniques have been used in numerous real-world applications. An assumption of traditional machine learning methodologies is the training data and testing data are taken from the same domain, such that the input feature space and data distribution characteristics are the same. However, in some real-world machine learning scenarios, this assumption does not hold. There are cases where training data is expensive or difficult to collect. Therefore, there is a need to create high-performance learners trained with more easily obtained data from different domains. This methodology is referred to as transfer learning. This survey paper formally defines transfer learning, presents information on current solutions, and reviews applications applied to transfer learning. Lastly, there is information listed on software downloads for various transfer learning solutions and a discussion of possible future research work. The transfer learning solutions surveyed are independent of data size and can be applied to big data environments.},
author = {Weiss, Karl and Khoshgoftaar, Taghi M. and Wang, Ding Ding},
doi = {10.1186/s40537-016-0043-6},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Weiss, Khoshgoftaar, Wang - 2016 - A survey of transfer learning.pdf:pdf},
issn = {21961115},
journal = {Journal of Big Data},
keywords = {Data mining,Domain adaptation,Machine learning,Survey,Transfer learning},
month = {dec},
number = {1},
pages = {9},
publisher = {SpringerOpen},
title = {{A survey of transfer learning}},
url = {http://journalofbigdata.springeropen.com/articles/10.1186/s40537-016-0043-6},
volume = {3},
year = {2016}
}
@inproceedings{Huang2017,
abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
archivePrefix = {arXiv},
arxivId = {1608.06993},
author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
eprint = {1608.06993},
month = {aug},
pages = {2261--2269},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Densely Connected Convolutional Networks}},
url = {http://arxiv.org/abs/1608.06993},
volume = {2017-Janua},
year = {2016}
}
@inproceedings{Awan2016,
abstract = {Emerging paradigms like High Performance Data Analytics (HPDA) and Deep Learning (DL) pose at least two new design challenges for existing MPI runtimes. First, these paradigms require an efficient support for communicating unusually large messages across processes. And second, the communication buffers used by HPDA applications and DL frameworks generally reside on a GPU's memory. In this context, we observe that conventional MPI runtimes have been optimized over decades to achieve lowest possible communication latency for relatively smaller message sizes (up-to 1 Megabyte) and that too for CPU memory buffers. With the advent of CUDA-Aware MPI runtimes, a lot of research has been conducted to improve performance of GPU buffer based communication. However, little exists in current state of the art that deals with very large message communication of GPU buffers. In this paper, we investigate these new challenges by analyzing the performance bottlenecks in existing CUDA-Aware MPI runtimes like MVAPICH2-GDR, and propose hierarchical collective designs to improve communication latency of the MPI-Bcast primitive by exploiting a new communication library called NCCL. To the best of our knowledge, this is the first work that addresses these new requirements where GPU buffers are used for communication with message sizes surpassing hundreds of megabytes. We highlight the design challenges for our work along with the details of design and implementation. In addition, we provide a comprehensive performance evaluation using a Micro-benchmark and a CUDA-Aware adaptation of Microsoft CNTK DL framework. We report up to 47{\%} improvement in training time for CNTK using the proposed hierarchical MPI-Bcast design.},
address = {New York, New York, USA},
author = {Awan, A. A. and Hamidouche, K. and Venkatesh, A. and Panda, D. K.},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/2966884.2966912},
isbn = {9781450342346},
month = {sep},
pages = {15--22},
publisher = {Association for Computing Machinery},
title = {{Efficient large message broadcast using NCCL and CUDA-Aware MPI for deep learning}},
url = {http://dl.acm.org/citation.cfm?doid=2966884.2966912},
volume = {25-28-Sept},
year = {2016}
}
@article{Malhat2020,
abstract = {Several approaches for instance selection have been put forward as a primary step to increase the efficiency and accuracy of algorithms applied to mine big data. The instance selection task scales indeed big data down by removing irrelevant, redundant, and unreliable data, which, in turn, reduces the computational resources necessary for completing the mining task. The local density-based approaches are recently acknowledged as feasible approaches in terms of reduction rate, effectiveness, and computation time metrics. However, these approaches endure low classification accuracy results compared with other approaches. In this manuscript, we propose a new layered and operational approach to address these limitations as well as advance the state-of-the-art by balancing among classification accuracy, reduction rate, and time complexity. We commence by designing a new algorithm (called GDIS) that selects most relevant instances using a global density and relevance functions. This enable us to consider a global view overall a data set to get a better classification accuracy results than current density-based approaches. We design another novel algorithm (called EGDIS), which maintains the effectiveness results of the GDIS algorithm while improving reduction rate results. Moreover, we compare our algorithms against three state-of-the-art algorithms to validate their performance. We develop a Java toolkit called ISTK on the top of the GDIS and EGDIS algorithms, the density-based approaches, and the state-of-the-art algorithms. We also develop a suitable user interface and its management and validation capabilities to ease-of-use and visualize results and data sets. We evaluate and test the performance of our algorithms in terms of four metrics (reduction rate, classification accuracy, effectiveness, and computation time) using twenty-four standard data sets and conduct an intensive set of experiments. The experimental results proved that the GDIS algorithm outperforms the density-based approaches in terms of classification accuracy and effectiveness, the EGDIS algorithm outperforms the density-based approaches in terms of reduction rate and effectiveness, and the GDIS and EGDIS algorithms outperform the state-of-the-art algorithms in terms of achieving a good results in both the effectiveness and computation time metrics. We finally test the scalability and compute experimentally the polynomial-time complexity of our algorithms.},
author = {Malhat, Mohamed and Menshawy, Mohamed El and Mousa, Hamdy and Sisi, Ashraf El},
doi = {10.1016/j.eswa.2020.113297},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Malhat et al. - 2020 - A new approach for instance selection Algorithms, evaluation, and comparisons(2).pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Big data,Data mining,Global density function,Instance selection,Time complexity},
month = {jul},
pages = {113297},
publisher = {Elsevier Ltd},
title = {{A new approach for instance selection: Algorithms, evaluation, and comparisons}},
volume = {149},
year = {2020}
}
@article{Ba2014,
abstract = {We present an attention-based model for recognizing multiple objects in images. The proposed model is a deep recurrent neural network trained with reinforcement learning to attend to the most relevant regions of the input image. We show that the model learns to both localize and recognize multiple objects despite being given only class labels during training. We evaluate the model on the challenging task of transcribing house number sequences from Google Street View images and show that it is both more accurate than the state-of-the-art convolutional networks and uses fewer parameters and less computation.},
archivePrefix = {arXiv},
arxivId = {1412.7755},
author = {Ba, Jimmy and Mnih, Volodymyr and Kavukcuoglu, Koray},
eprint = {1412.7755},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/MULTIPLE OBJECT RECOGNITION WITH VISUAL ATTENTION.pdf:pdf},
pages = {1--10},
title = {{Multiple Object Recognition with Visual Attention}},
url = {http://arxiv.org/abs/1412.7755},
year = {2014}
}
@inproceedings{Wiatowski2018,
abstract = {Deep convolutional neural networks (DCNNs) have led to breakthrough results in numerous practical machine learning tasks, such as classification of images in the ImageNet data set, control-policy-learning to play Atari games or the board game Go, and image captioning. Many of these applications first perform feature extraction and then feed the results thereof into a classifier. The mathematical analysis of DCNNs for feature extraction was initiated by Mallat, 2012. Specifically, Mallat considered so-called scattering networks based on a wavelet transform followed by the modulus non-linearity in each network layer, and proved translation invariance (asymptotically in the wavelet scale parameter) and deformation stability of the corresponding feature extractor. This paper complements Mallat's results by developing a theory that encompasses general convolutional transforms, or in more technical parlance, general semi-discrete frames (including Weyl-Heisenberg filters, curvelets, shearlets, ridgelets, wavelets, and learned filters), general Lipschitz-continuous non-linearities (e.g., rectified linear units, shifted logistic sigmoids, hyperbolic tangents, and modulus functions), and general Lipschitz-continuous pooling operators emulating, e.g., sub-sampling and averaging. In addition, all of these elements can be different in different network layers. For the resulting feature extractor, we prove a translation invariance result of vertical nature in the sense of the features becoming progressively more translation-invariant with increasing network depth, and we establish deformation sensitivity bounds that apply to signal classes such as, e.g., band-limited functions, cartoon functions, and Lipschitz functions.},
archivePrefix = {arXiv},
arxivId = {1512.06293},
author = {Wiatowski, Thomas and Bolcskei, Helmut},
booktitle = {IEEE Transactions on Information Theory},
doi = {10.1109/TIT.2017.2776228},
eprint = {1512.06293},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Wiatowski, Bolcskei - 2018 - A Mathematical Theory of Deep Convolutional Neural Networks for Feature Extraction.pdf:pdf},
issn = {00189448},
keywords = {Machine learning,deep convolutional neural networks,feature extraction,frame theory,scattering networks},
month = {mar},
number = {3},
pages = {1845--1866},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{A Mathematical Theory of Deep Convolutional Neural Networks for Feature Extraction}},
volume = {64},
year = {2018}
}
@article{Wang2017,
abstract = {The GANs promote an adversarive game to approximate complex and jointed example probability. The networks driven by noise generate fake examples to approximate realistic data distributions. Later the conditional GAN merges prior-conditions as input in order to transfer attribute vectors to the corresponding data. However, the CGAN is not designed to deal with the high dimension conditions since indirect guide of the learning is inefficiency. In this paper, we proposed a network ResGAN to generate fine images in terms of extremely degenerated images. The coarse images aligned to attributes are embedded as the generator inputs and classifier labels. In generative network, a straight path similar to the Resnet is cohered to directly transfer the coarse images to the higher layers. And adversarial training is circularly implemented to prevent degeneration of the generated images. Experimental results of applying the ResGAN to datasets MNIST, CIFAR10/100 and CELEBA show its higher accuracy to the state-of-art GANs.},
archivePrefix = {arXiv},
arxivId = {1707.04881},
author = {Wang, Meng and Li, Huafeng and Li, Fang},
eprint = {1707.04881},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Wang, Li, Li - 2017 - Generative adversarial network based on resnet for conditional image restoration Paper jc--- Generative Adversaria.pdf:pdf},
title = {{Generative Adversarial Network based on Resnet for Conditional Image Restoration}},
url = {https://arxiv.org/pdf/1707.04881.pdf http://arxiv.org/abs/1707.04881},
year = {2017}
}
@article{Peters2018,
abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
archivePrefix = {arXiv},
arxivId = {1802.05365},
author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
eprint = {1802.05365},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Peters et al. - 2018 - Deep contextualized word representations.pdf:pdf},
month = {feb},
title = {{Deep contextualized word representations}},
url = {http://arxiv.org/abs/1802.05365},
year = {2018}
}
@techreport{Vinet2011,
abstract = {We study a family of "classical" orthogonal polynomials which satisfy (apart from a 3-term recurrence relation) an eigenvalue problem with a differential operator of Dunkl-type. These polynomials can be obtained from the little {\$}q{\$}-Jacobi polynomials in the limit {\$}q=-1{\$}. We also show that these polynomials provide a nontrivial realization of the Askey-Wilson algebra for {\$}q=-1{\$}.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Vinet, Luc and Zhedanov, Alexei},
booktitle = {Journal of Physics A: Mathematical and Theoretical},
doi = {10.1088/1751-8113/44/8/085201},
eprint = {arXiv:1011.1669v3},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Goodfellow et al. - 2014 - Generative Adversarial Nets(2).pdf:pdf},
isbn = {9780300023077},
issn = {17518113},
number = {8},
pages = {iii},
pmid = {15645445},
title = {{A 'missing' family of classical orthogonal polynomials}},
url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets https://papers.nips.cc/paper/5423-generative-adversarial-nets{\%}0Ahttp://doi.wiley.com/10.1002/9781118472507.fmatter{\%}0Ahttp://linkinghub.elsevier.com/retrieve/pii/B9780408001090500018},
volume = {44},
year = {2011}
}
@inproceedings{Fan2017,
abstract = {Generation of 3D data by deep neural network has been attracting increasing attention in the research community. The majority of extant works resort to regular representations such as volumetric grids or collection of images; however, these representations obscure the natural invariance of 3D shapes under geometric transformations and also suffer from a number of other issues. In this paper we address the problem of 3D reconstruction from a single image, generating a straight-forward form of output -- point cloud coordinates. Along with this problem arises a unique and interesting issue, that the groundtruth shape for an input image may be ambiguous. Driven by this unorthodox output form and the inherent ambiguity in groundtruth, we design architecture, loss function and learning paradigm that are novel and effective. Our final solution is a conditional shape sampler, capable of predicting multiple plausible 3D point clouds from an input image. In experiments not only can our system outperform state-of-the-art methods on single image based 3d reconstruction benchmarks; but it also shows a strong performance for 3d shape completion and promising ability in making multiple plausible predictions.},
archivePrefix = {arXiv},
arxivId = {1612.00603},
author = {Fan, Haoqiang and Su, Hao and Guibas, Leonidas},
booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
doi = {10.1109/CVPR.2017.264},
eprint = {1612.00603},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Fan, Su, Guibas - 2016 - A Point Set Generation Network for 3D Object Reconstruction from a Single Image.pdf:pdf},
isbn = {9781538604571},
month = {dec},
pages = {2463--2471},
title = {{A point set generation network for 3D object reconstruction from a single image}},
url = {http://arxiv.org/abs/1612.00603},
volume = {2017-Janua},
year = {2017}
}
@article{Jorda2019,
abstract = {Convolutional neural networks (CNNs) have recently attracted considerable attention due to their outstanding accuracy in applications, such as image recognition and natural language processing. While one advantage of the CNNs over other types of neural networks is their reduced computational cost, faster execution is still desired for both training and inference. Since convolution operations pose most of the execution time, multiple algorithms were and are being developed with the aim of accelerating this type of operations. However, due to the wide range of convolution parameter configurations used in the CNNs and the possible data type representations, it is not straightforward to assess in advance which of the available algorithms will be the best performing in each particular case. In this paper, we present a performance evaluation of the convolution algorithms provided by the cuDNN, the library used by most deep learning frameworks for their GPU operations. In our analysis, we leverage the convolution parameter configurations from widely used the CNNs and discuss which algorithms are better suited depending on the convolution parameters for both 32 and 16-bit floating-point (FP) data representations. Our results show that the filter size and the number of inputs are the most significant parameters when selecting a GPU convolution algorithm for 32-bit FP data. For 16-bit FP, leveraging specialized arithmetic units (NVIDIA Tensor Cores) is key to obtain the best performance.},
author = {Jorda, Marc and Valero-Lara, Pedro and Pena, Antonio J.},
doi = {10.1109/ACCESS.2019.2918851},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Jorda, Valero-Lara, Pena - 2019 - Performance Evaluation of cuDNN Convolution Algorithms on NVIDIA Volta GPUs.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {GPU,Neural network,convolution,cuDNN,deep learning,volta},
pages = {70461--70473},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Performance Evaluation of cuDNN Convolution Algorithms on NVIDIA Volta GPUs}},
volume = {7},
year = {2019}
}
@article{Maaten2008,
abstract = {We present a new technique called “t-SNE” that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence theway in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualiza- tions produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
author = {van der Maaten, Laurens and Hinton, Geoffrey},
doi = {10.1007/s10479-011-0841-3},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Maaten, Hinton - 2008 - Visualizing Data using t-SNE.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {dimensionality reduction,embedding algorithms,manifold learning,multidimensional scaling,visualization},
pages = {2579--2605},
pmid = {20652508},
title = {{Visualizing Data using t-SNE}},
url = {http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=pubmed{\&}cmd=Retrieve{\&}dopt=AbstractPlus{\&}list{\_}uids=7911431479148734548related:VOiAgwMNy20J},
volume = {9},
year = {2008}
}
@book{Courville2016,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
booktitle = {MIT press},
pages = {304},
title = {{Deep Learning}},
url = {https://books.google.co.id/books?hl=en{\&}lr={\&}id=omivDQAAQBAJ{\&}oi=fnd{\&}pg=PR5{\&}dq=deep+learning{\&}ots=MMV49uoHUY{\&}sig=1IlH9THiE2M{\_}I8{\_}q{\_}G6g6bpWCxU{\&}redir{\_}esc=y{\#}v=onepage{\&}q=deep learning{\&}f=false},
year = {2016}
}
@inproceedings{Paul2012,
abstract = {The strategy of face recognition involves the examination of facial features in a picture, recognizing those features and matching them to 1 of the many faces in the database. There are lots of algorithms effective at performing face recognition, such as for instance: Principal Component Analysis, Discrete Cosine Transform, 3D acceptance methods, Gabor Wavelets method etc. This work has centered on Principal Component Analysis (PCA) method for face recognition in an efficient manner. There are numerous issues to take into account whenever choosing a face recognition method. The main element is: Accuracy, Time limitations, Process speed and Availiability. With one of these in minds PCA way of face recognition is selected because it is really a simplest and easiest approach to implement, extremely fast computation time. PCA (Principal Component Analysis) is an activity that extracts the absolute most relevant information within a face and then tries to construct a computational model that best describes it.},
author = {Kaur, Ramandeep and Himanshi, Er},
booktitle = {Souvenir of the 2015 IEEE International Advance Computing Conference, IACC 2015},
doi = {10.1109/IADCC.2015.7154774},
isbn = {9781479980475},
keywords = {Eigen Vector and feature extraction,Face Recognition,PCA},
pages = {585--589},
title = {{Face recognition using Principal Component Analysis}},
year = {2015}
}
@misc{Brighton2002,
abstract = {The basic nearest neighbour classifier suffers from the indiscriminate storage of all presented training instances. With a large database of instances classification response time can be slow. When noisy instances are present classification accuracy can suffer. Drawing on the large body of relevant work carried out in the past 30 years, we review the principle approaches to solving these problems. By deleting instances, both problems can be alleviated, but the criterion used is typically assumed to be all encompassing and effective over many domains. We argue against this position and introduce an algorithm that rivals the most successful existing algorithm. When evaluated on 30 different problems, neither algorithm consistently outperforms the other: consistency is very hard. To achieve the best results, we need to develop mechanisms that provide insights into the structure of class definitions. We discuss the possibility of these mechanisms and propose some initial measures that could be useful for the data miner.},
author = {Brighton, Henry and Mellish, Chris},
booktitle = {Data Mining and Knowledge Discovery},
doi = {10.1023/A:1014043630878},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Brighton, Mellish - 2002 - Advances in instance selection for instance-based learning algorithms.pdf:pdf},
issn = {13845810},
keywords = {Forgetting,Instance selection,Instance-based learning,Pruning},
number = {2},
pages = {153--172},
publisher = {Springer},
title = {{Advances in instance selection for instance-based learning algorithms}},
volume = {6},
year = {2002}
}
@article{Bolukbasi2016,
abstract = {The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between between the words receptionist and female, while maintaining desired associations such as between the words queen and female. We define metrics to quantify both direct and indirect gender biases in embeddings, and develop algorithms to "debias" the embedding. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.},
archivePrefix = {arXiv},
arxivId = {1607.06520},
author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James and Saligrama, Venkatesh and Kalai, Adam},
eprint = {1607.06520},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Bolukbasi et al. - 2016 - Man is to Computer Programmer as Woman is to Homemaker Debiasing Word Embeddings(2).pdf:pdf},
month = {jul},
title = {{Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings}},
url = {http://arxiv.org/abs/1607.06520},
year = {2016}
}
@techreport{Kar,
abstract = {We present a learnt system for multi-view stereopsis. In contrast to recent learning based methods for 3D reconstruction, we leverage the underlying 3D geometry of the problem through feature projection and unprojection along viewing rays. By formulating these operations in a differentiable manner, we are able to learn the system end-to-end for the task of metric 3D reconstruction. End-to-end learning allows us to jointly reason about shape priors while conforming to geometric constraints, enabling reconstruction from much fewer images (even a single image) than required by classical approaches as well as completion of unseen surfaces. We thoroughly evaluate our approach on the ShapeNet dataset and demonstrate the benefits over classical approaches and recent learning based methods.},
author = {Kar, Abhishek and H{\"{a}}ne, Christian and Malik, Jitendra},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Kar, H{\"{a}}ne, Malik - Unknown - Learning a Multi-View Stereo Machine.pdf:pdf},
title = {{Learning a Multi-View Stereo Machine}},
url = {http://papers.nips.cc/paper/6640-learning-a-multi-view-stereo-machine.pdf}
}
@incollection{Oleynikova2016,
abstract = {How to represent a map of the environment is a key question of robotics. In this paper, we focus on suggesting a representation well-suited for online map building from vision-based data and online planning in 3D. We propose to combine a commonly-used representation in computer graphics and surface reconstruction, projective Truncated Signed Distance Field (TSDF), with a representation frequently used for colli-sion checking and collision costs in planning, Euclidean Signed Distance Field (ESDF), and validate this combined approach in simulation. We argue that this type of map is better-suited for robotic applications than existing representations.},
author = {Oleynikova, Helen and Millane, Alex and Taylor, Zachary and Galceran, Enric and Nieto, Juan and Siegwart, Roland},
booktitle = {RSS 2016 Workshop: Geometry and Beyond - Representations, Physics, and Scene Understanding for Robotics},
doi = {10.3929/ethz-a-010820134},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Oleynikova et al. - Unknown - Signed Distance Fields A Natural Representation for Both Mapping and Planning.pdf:pdf},
title = {{Signed Distance Fields: A Natural Representation for Both Mapping and Planning}},
url = {https://rss16-representations.mit.edu/papers/BeyondGeometryRSSW16{\_}2{\_}CameraReadySubmission{\_}Oleynikova.pdf},
year = {2016}
}
@inproceedings{He2016,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2016.90},
eprint = {1512.03385},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
pages = {770--778},
title = {{Deep residual learning for image recognition}},
url = {http://image-net.org/challenges/LSVRC/2015/},
volume = {2016-Decem},
year = {2016}
}
@article{Curless1996,
abstract = {A number of techniques have been developed for reconstructing surfaces by integrating groups of aligned range images. A desirable set of properties for such algorithms includes: incremental updating, representation of directional uncertainty, the ability to fill gaps in the reconstruction, and robustness in the presence of outliers. Prior algorithms possess subsets of these properties. In this paper, we present a volumetric method for integrating range images that possesses all of these properties. Our volumetric representation consists of a cumulative weighted signed distance function. Working with one range image at a time, we first scan-convert it to a distance function, then combine this with the data already acquired using a simple additive scheme. To achieve space efficiency, we employ a run-length encoding of the volume. To achieve time efficiency, we resample the range image to align with the voxel grid and traverse the range and voxel scanlines synchronously. We generate the final manifold by extracting an isosurface from the volumetric grid. We show that under certain assumptions, this isosurface is optimal in the least squares sense. To fill gaps in the model, we tessellate over the boundaries between regions seen to be empty and regions never observed. Using this method, we are able to integrate a large number of range images (as many as 70) yielding seamless, high-detail models of up to 2.6 million triangles.},
author = {Curless, Brian and Levoy, Marc},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Curless - 1996 - A Volumetric Method for Building Complex Models from Range Images.pdf:pdf},
journal = {Stanford University},
title = {{A Volumetric Method for Building Complex Models from Range Images}},
url = {http://papers.cumincad.org/cgi-bin/works/Show?2ca3},
year = {1996}
}
@inproceedings{Quattoni2010,
abstract = {Indoor scene recognition is a challenging open problem in high level vision. Most scene recognition models that work well for outdoor scenes perform poorly in the indoor domain. The main difficulty is that while some indoor scenes (e.g. corridors) can be well characterized by global spatial properties, others (e.g, bookstores) are better characterized by the objects they contain. More generally, to address the indoor scenes recognition problem we need a model that can exploit local and global discriminative information. In this paper we propose a prototype based model that can successfully combine both sources of information. To test our approach we created a dataset of 67 indoor scenes categories (the largest available) covering a wide range of domains. The results show that our approach can significantly outperform a state of the art classifier for the task.},
author = {Quattoni, Ariadna and Torralba, Antonio},
doi = {10.1109/cvpr.2009.5206537},
month = {mar},
pages = {413--420},
publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
title = {{Recognizing indoor scenes}},
year = {2010}
}
@article{Aha1991,
abstract = {Abstract. Storing and using specific instances improves the performance of several supervised learning algorithms. These include algorithms that learn decision trees, classification rules, and distributed networks. However, no investigation has analyzed algorithms that use only specific instances to solve incremental learning tasks. In this paper, we describe a framework and methodology, called instance-based learning, that generates classification predictions using only specific instances. Instance-based learning algorithms do not maintain a set of abstractions derived from specific instances. This approach extends the nearest neighbor algorithm, which has large storage requirements. We describe how storage requirements can be significantly reduced with, at most, minor sacrifices in learning rate and classification accuracy. While the storage-reducing algorithm performs well on several realworld databases, its performance degrades rapidly with the level of attribute noise in training instances. Therefore, we extended it with a significance test to distinguish noisy instances. This extended algorithm's performance degrades gracefully with increasing noise levels and compares favorably with a noise-tolerant decision tree algorithm.},
author = {Aha, David W. and Kibler, Dennis and Albert, Marc K.},
doi = {10.1007/bf00153759},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Aha, Kibler, Albert - 1991 - Instance-based learning algorithms.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {Artificial Intelligence,Control,Mechatronics,Natural Language Processing (NLP),Robotics,Simulation and Modeling},
month = {jan},
number = {1},
pages = {37--66},
publisher = {Springer Nature},
title = {{Instance-based learning algorithms}},
volume = {6},
year = {1991}
}
@techreport{Mikolova,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {1301.3781v3},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
eprint = {1301.3781v3},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Mikolov et al. - Unknown - Efficient Estimation of Word Representations in Vector Space.pdf:pdf},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {http://ronan.collobert.com/senna/}
}
@techreport{Arjovsky,
abstract = {The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen-erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first section introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a practical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.},
archivePrefix = {arXiv},
arxivId = {1701.04862v1},
author = {Arjovsky, Martin and Bottou, L{\'{e}}on},
eprint = {1701.04862v1},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Arjovsky, Bottou - Unknown - TOWARDS PRINCIPLED METHODS FOR TRAINING GENERATIVE ADVERSARIAL NETWORKS(2).pdf:pdf},
title = {{TOWARDS PRINCIPLED METHODS FOR TRAINING GENERATIVE ADVERSARIAL NETWORKS}},
url = {https://arxiv.org/pdf/1701.04862.pdf}
}
@inproceedings{Kumar2010,
abstract = {Latent variable models are a powerful tool for addressing several tasks in machine learning. However, the algorithms for learning the parameters of latent variable models are prone to getting stuck in a bad local optimum. To alleviate this problem, we build on the intuition that, rather than considering all samples simultaneously, the algorithm should be presented with the training data in a meaningful order that facilitates learning. The order of the samples is determined by how easy they are. The main challenge is that often we are not provided with a readily computable measure of the easiness of samples. We address this issue by proposing a novel, iterative self-paced learning algorithm where each iteration simultaneously selects easy samples and learns a new parameter vector. The number of samples selected is governed by a weight that is annealed until the entire training data has been considered. We empirically demonstrate that the self-paced learning algorithm outperforms the state of the art method for learning a latent structural SVM on four applications: object localization, noun phrase coreference, motif finding and handwritten digit recognition.},
author = {Kumar, M. Pawan and Packer, Benjamin and Koller, Daphne},
booktitle = {Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010, NIPS 2010},
isbn = {9781617823800},
pages = {1189--1197},
title = {{Self-paced learning for latent variable models}},
year = {2010}
}
@article{Qi2017,
abstract = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds and well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
archivePrefix = {arXiv},
arxivId = {1612.00593},
author = {Qi, Charles R. and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.},
doi = {10.1109/CVPR.2017.16},
eprint = {1612.00593},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Qi et al. - 2017 - PointNet Deep learning on point sets for 3D classification and segmentation.pdf:pdf},
isbn = {9781538604571},
issn = {1063-6919},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {77--85},
pmid = {16378701},
title = {{PointNet: Deep learning on point sets for 3D classification and segmentation}},
url = {http://arxiv.org/abs/1612.00593},
volume = {2017-Janua},
year = {2017}
}
@article{Zhang2018,
abstract = {In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.},
archivePrefix = {arXiv},
arxivId = {1805.08318},
author = {Zhang, Han and Goodfellow, Ian and Metaxas, Dimitris and Odena, Augustus},
eprint = {1805.08318},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Self-Attention Generative Adversarial Networks.pdf:pdf;:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2018 - Self-Attention Generative Adversarial Networks.pdf:pdf},
title = {{Self-Attention Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1805.08318},
year = {2018}
}
@techreport{Chen,
abstract = {We advocate the use of implicit fields for learning gen-erative models of shapes and introduce an implicit field de-coder, called IM-NET, for shape generation, aimed at improving the visual quality of the generated shapes. An implicit field assigns a value to each point in 3D space, so that a shape can be extracted as an iso-surface. IM-NET is trained to perform this assignment by means of a binary classifier. Specifically, it takes a point coordinate, along with a feature vector encoding a shape, and outputs a value which indicates whether the point is outside the shape or not. By replacing conventional decoders by our implicit de-coder for representation learning (via IM-AE) and shape generation (via IM-GAN), we demonstrate superior results for tasks such as generative shape modeling, interpolation, and single-view 3D reconstruction, particularly in terms of visual quality. Code and supplementary material are available at https://github.com/czq142857/implicit-decoder.},
archivePrefix = {arXiv},
arxivId = {1812.02822v4},
author = {Chen, Zhiqin and Zhang, Hao},
eprint = {1812.02822v4},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Chen, Zhang - Unknown - Learning Implicit Fields for Generative Shape Modeling(3).pdf:pdf},
title = {{Learning Implicit Fields for Generative Shape Modeling}},
url = {https://github.com/czq142857/implicit-decoder.}
}
@article{Song2020,
abstract = {With the wide application of Light Detection and Ranging (LiDAR) in the collection of high-precision environmental point cloud information, three-dimensional (3D) object classification from point clouds has become an important research topic. However, the characteristics of LiDAR point clouds, such as unstructured distribution, disordered arrangement, and large amounts of data, typically result in high computational complexity and make it very difficult to classify 3D objects. Thus, this paper proposes a Convolutional Neural Network (CNN)-based 3D object classification method using the Hough space of LiDAR point clouds to overcome these problems. First, object point clouds are transformed into Hough space using a Hough transform algorithm, and then the Hough space is rasterized into a series of uniformly sized grids. The accumulator count in each grid is then computed and input to a CNN model to classify 3D objects. In addition, a semi-automatic 3D object labeling tool is developed to build a LiDAR point clouds object labeling library for four types of objects (wall, bush, pedestrian, and tree). After initializing the CNN model, we apply a dataset from the above object labeling library to train the neural network model offline through a large number of iterations. Experimental results demonstrate that the proposed method achieves object classification accuracy of up to 93.3{\%} on average.},
author = {Song, Wei and Zhang, Lingfeng and Tian, Yifei and Fong, Simon and Liu, Jinming and Gozho, Amanda},
doi = {10.1186/s13673-020-00228-8},
issn = {21921962},
journal = {Human-centric Computing and Information Sciences},
keywords = {3D object classification,CNN,Hough space,LiDAR point clouds},
month = {dec},
number = {1},
pages = {1--14},
publisher = {Springer},
title = {{CNN-based 3D object classification using Hough space of LiDAR point clouds}},
url = {https://link.springer.com/articles/10.1186/s13673-020-00228-8 https://link.springer.com/article/10.1186/s13673-020-00228-8},
volume = {10},
year = {2020}
}
@article{Krizhevsky2017,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%}, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used nonsaturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
doi = {10.1145/3065386},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Krizhevsky, Sutskever, Hinton - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
issn = {15577317},
journal = {Communications of the ACM},
number = {6},
pages = {84--90},
title = {{ImageNet classification with deep convolutional neural networks}},
url = {http://code.google.com/p/cuda-convnet/},
volume = {60},
year = {2017}
}
@techreport{Li2017,
abstract = {Convolutional neural networks (CNNs) have achieved breakthrough performance in many pattern recognition tasks. In order to distinguish the reliable data from the noisy and confusing data, we improve CNNs with self-paced learning (SPL) for enhancing the learning robustness of CNNs. In the proposed self-paced convolutional network (SPCN), each sample is assigned to a weight to reflect the easiness of the sample. Then a dynamic self-paced function is incorporated into the leaning objective of CNN to jointly learn the parameters of CNN and the latent weight variable. SPCN learns the samples from easy to complex and the sample weights can dynamically control the learning rates for converging to better values. To gain more insights of SPCN, theoretical studies are conducted to show that SPCN converges to a stationary solution and is robust to the noisy and confusing data. Experimental results on MNIST and rectangles datasets demonstrate that the proposed method out-performs baseline methods.},
author = {Li, Hao and Gong, Maoguo},
booktitle = {ijcai.org},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Li, Gong - 2017 - Self-paced Convolutional Neural Networks.pdf:pdf},
keywords = {Machine Learning: Machine Learning,Machine Learning: Neural Networks},
title = {{Self-paced Convolutional Neural Networks}},
url = {https://www.ijcai.org/proceedings/2017/0293.pdf},
year = {2017}
}
@article{Park2019,
abstract = {Computer graphics, 3D computer vision and robotics communities have produced multiple approaches to representing 3D geometry for rendering and reconstruction. These provide trade-offs across fidelity, efficiency and compression capabilities. In this work, we introduce DeepSDF, a learned continuous Signed Distance Function (SDF) representation of a class of shapes that enables high quality shape representation, interpolation and completion from partial and noisy 3D input data. DeepSDF, like its classical counterpart, represents a shape's surface by a continuous volumetric field: the magnitude of a point in the field represents the distance to the surface boundary and the sign indicates whether the region is inside (-) or outside (+) of the shape, hence our representation implicitly encodes a shape's boundary as the zero-level-set of the learned function while explicitly representing the classification of space as being part of the shapes interior or not. While classical SDF's both in analytical or discretized voxel form typically represent the surface of a single shape, DeepSDF can represent an entire class of shapes. Furthermore, we show state-of-the-art performance for learned 3D shape representation and completion while reducing the model size by an order of magnitude compared with previous work.},
archivePrefix = {arXiv},
arxivId = {1901.05103},
author = {Park, Jeong Joon and Florence, Peter and Straub, Julian and Newcombe, Richard and Lovegrove, Steven},
eprint = {1901.05103},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Park et al. - 2019 - DeepSDF Learning Continuous Signed Distance Functions for Shape Representation(3).pdf:pdf},
month = {jan},
title = {{DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation}},
url = {https://arxiv.org/abs/1901.05103 http://arxiv.org/abs/1901.05103},
year = {2019}
}
@article{Liu2018,
abstract = {Hyperspectral image classification has become a research focus in recent literature. However, well-designed features are still open issues that impact on the performance of classifiers. In this paper, a novel supervised deep feature extraction method based on siamese convolutional neural network (S-CNN) is proposed to improve the performance of hyperspectral image classification. First, a CNN with five layers is designed to directly extract deep features from hyperspectral cube, where the CNN can be intended as a nonlinear transformation function. Then, the siamese network composed by two CNNs is trained to learn features that show a low intraclass and high interclass variability. The important characteristic of the presented approach is that the S-CNN is supervised with a margin ranking loss function, which can extract more discriminative features for classification tasks. To demonstrate the effectiveness of the proposed feature extraction method, the features extracted from three widely used hyperspectral data sets are fed into a linear support vector machine (SVM) classifier. The experimental results demonstrate that the proposed feature extraction method in conjunction with a linear SVM classifier can obtain better classification performance than that of the conventional methods.},
author = {Liu, Bing and Yu, Xuchu and Zhang, Pengqiang and Yu, Anzhu and Fu, Qiongying and Wei, Xiangpo},
doi = {10.1109/TGRS.2017.2769673},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Liu et al. - 2018 - Supervised deep feature extraction for hyperspectral image classification.pdf:pdf},
issn = {01962892},
journal = {IEEE Transactions on Geoscience and Remote Sensing},
keywords = {Convolutional neural network (CNN),Deep feature extraction,Hyperspectral image classification,Siamese network,Support vector machine (SVM)},
month = {apr},
number = {4},
pages = {1909--1921},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Supervised deep feature extraction for hyperspectral image classification}},
volume = {56},
year = {2018}
}
@article{Meng2016,
abstract = {Self-paced learning (SPL) is a recently raised methodology designed through simulating the learning principle of humans/animals. A variety of SPL realization schemes have been designed for different computer vision and pattern recognition tasks, and empirically substantiated to be effective in these applications. However, the investigation on its theoretical insight is still a blank. To this issue, this study attempts to provide some new theoretical understanding under the SPL scheme. Specifically, we prove that the solving strategy on SPL accords with a majorization minimization algorithm implemented on a latent objective function. Furthermore, we find that the loss function contained in this latent objective has a similar configuration with non-convex regularized penalty (NSPR) known in statistics and machine learning. Such connection inspires us discovering more intrinsic relationship between SPL regimes and NSPR forms, like SCAD, LOG and EXP. The robustness insight under SPL can then be finely explained. We also analyze the capability of SPL on its easy loss prior embedding property, and provide an insightful interpretation to the effectiveness mechanism under previous SPL variations. Besides, we design a group-partial-order loss prior, which is especially useful to weakly labeled large-scale data processing tasks. Through applying SPL with this loss prior to the FCVID dataset, which is currently one of the biggest manually annotated video dataset, our method achieves state-of-the-art performance beyond previous methods, which further helps supports the proposed theoretical arguments.},
archivePrefix = {arXiv},
arxivId = {1511.06049},
author = {Meng, Deyu and Zhao, Qian and Jiang, Lu},
eprint = {1511.06049},
isbn = {1511.06049v2},
keywords = {Self-paced learning,curriculum learning,multimedia event detection,non-convex regularized penalty},
title = {{What Objective Does Self-paced Learning Indeed Optimize?}},
url = {http://arxiv.org/abs/1511.06049},
year = {2015}
}
@article{Mun2017,
abstract = {We present a framework to analyze various aspects of models for video question answering (VideoQA) using customizable synthetic datasets, which are constructed automatically from gameplay videos. Our work is motivated by the fact that existing models are often tested only on datasets that require excessively high-level reasoning or mostly contain instances accessible through single frame inferences. Hence, it is difficult to measure capacity and flexibility of trained models, and existing techniques often rely on ad-hoc implementations of deep neural networks without clear insight into datasets and models. We are particularly interested in understanding temporal relationships between video events to solve VideoQA problems; this is because reasoning temporal dependency is one of the most distinct components in videos from images. To address this objective, we automatically generate a customized synthetic VideoQA dataset using {\{}$\backslash$em Super Mario Bros.{\}} gameplay videos so that it contains events with different levels of reasoning complexity. Using the dataset, we show that properly constructed datasets with events in various complexity levels are critical to learn effective models and improve overall performance.},
author = {Mun, Jonghwan and Seo, Paul Hongsuck and Jung, Ilchae and Han, Bohyung},
doi = {10.1109/ICCV.2017.312},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/MarioQA-Answering Questions by Watching Gameplay Videos.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2886--2894},
title = {{MarioQA: Answering Questions by Watching Gameplay Videos}},
volume = {2017-Octob},
year = {2017}
}
@article{Sergeev2018,
abstract = {Training modern deep learning models requires large amounts of computation, often provided by GPUs. Scaling computation from one GPU to many can enable much faster training and research progress but entails two complications. First, the training library must support inter-GPU communication. Depending on the particular methods employed, this communication may entail anywhere from negligible to significant overhead. Second, the user must modify his or her training code to take advantage of inter-GPU communication. Depending on the training library's API, the modification required may be either significant or minimal. Existing methods for enabling multi-GPU training under the TensorFlow library entail non-negligible communication overhead and require users to heavily modify their model-building code, leading many researchers to avoid the whole mess and stick with slower single-GPU training. In this paper we introduce Horovod, an open source library that improves on both obstructions to scaling: it employs efficient inter-GPU communication via ring reduction and requires only a few lines of modification to user code, enabling faster, easier distributed training in TensorFlow. Horovod is available under the Apache 2.0 license at https://github.com/uber/horovod},
archivePrefix = {arXiv},
arxivId = {1802.05799},
author = {Sergeev, Alexander and {Del Balso}, Mike},
eprint = {1802.05799},
month = {feb},
title = {{Horovod: fast and easy distributed deep learning in TensorFlow}},
url = {http://arxiv.org/abs/1802.05799},
year = {2018}
}
@article{Riquelme2003,
abstract = {This paper presents a new approach to finding representative patterns for dataset editing. The algorithm patterns by ordered projections (POP), has some interesting characteristics: Important reduction of the number of instances from the dataset; lower computational cost ($\Theta$(mn log n)) with respect to other typical algorithms due to the absence of distance calculations; conservation of the decision boundaries, especially from the point of view of the application of axis-parallel classifiers. POP works well in practice with both continuous and discrete attributes. The performance of POP is analysed in two ways: Percentage of reduction and classification. POP has been compared to IB2, ENN and SHRINK concerning the percentage of reduction and the computational cost. In addition, we have analysed the accuracy of k-NN and C4.5 after applying the reduction techniques. An extensive empirical study using datasets with continuous and discrete attributes from the UCI repository shows that POP is a valuable preprocessing method for the later application of any axis-parallel learning algorithm. {\textcopyright} 2002 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.},
author = {Riquelme, Jos{\'{e}} C. and Aguilar-Ruiz, Jes{\'{u}}s S. and Toro, Miguel},
doi = {10.1016/S0031-3203(02)00119-X},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Axis-parallel classifiers,Data mining,Pattern analysis,Preprocessing techniques},
month = {apr},
number = {4},
pages = {1009--1018},
publisher = {Elsevier Ltd},
title = {{Finding representative patterns with ordered projections}},
volume = {36},
year = {2003}
}
@article{Wu2019,
abstract = {In existing visual representation learning tasks, deep convolutional neural networks (CNNs) are often trained on images annotated with single tag, such as ImageNet. However, single tag annotation cannot describe all important contents of one image, and some useful visual information may be wasted during training. In this work, we propose to train CNNs from images annotated with multiple tags, to enhance the quality of visual representation of the trained CNN model. To this end, we build a large-scale multi-label image database with 18M images and 11K categories, dubbed Tencent ML-Images. We efficiently train the ResNet-101 model with multi-label outputs on Tencent ML-Images, taking 90 hours for 60 epochs, based on a large-scale distributed deep learning framework, i.e., TFplus. The good quality of the visual representation of the Tencent ML-Images checkpoint is verified through three transfer learning tasks, including single-label image classification on ImageNet and Caltech-256, object detection on PASCAL VOC 2007, and semantic segmentation on PASCAL VOC 2012. The Tencent ML-Images database, the checkpoints of ResNet-101, and all the training codes have been released at https://github.com/Tencent/tencent-ml-images. It is expected to promote other vision tasks in the research and industry community.},
archivePrefix = {arXiv},
arxivId = {1901.01703},
author = {Wu, Baoyuan and Chen, Weidong and Fan, Yanbo and Zhang, Yong and Hou, Jinlong and Liu, Jie and Zhang, Tong},
doi = {10.1109/ACCESS.2019.2956775},
eprint = {1901.01703},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Wu et al. - 2019 - Tencent ML-Images A Large-Scale Multi-Label Image Database for Visual Representation Learning.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Visual representation learning,image database,multi-label},
month = {jan},
pages = {172683--172693},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Tencent ML-Images: A Large-Scale Multi-Label Image Database for Visual Representation Learning}},
url = {http://arxiv.org/abs/1901.01703 http://dx.doi.org/10.1109/ACCESS.2019.2956775},
volume = {7},
year = {2019}
}
@article{Achlioptas2017,
abstract = {Three-dimensional geometric data offer an excellent domain for studying representation learning and generative modeling. In this paper, we look at geometric data represented as point clouds. We introduce a deep AutoEncoder (AE) network with state-of-the-art reconstruction quality and generalization ability. The learned representations outperform existing methods on 3D recognition tasks and enable shape editing via simple algebraic manipulations, such as semantic part editing, shape analogies and shape interpolation, as well as shape completion. We perform a thorough study of different generative models including GANs operating on the raw point clouds, significantly improved GANs trained in the fixed latent space of our AEs, and Gaussian Mixture Models (GMMs). To quantitatively evaluate generative models we introduce measures of sample fidelity and diversity based on matchings between sets of point clouds. Interestingly, our evaluation of generalization, fidelity and diversity reveals that GMMs trained in the latent space of our AEs yield the best results overall.},
archivePrefix = {arXiv},
arxivId = {1707.02392},
author = {Achlioptas, Panos and Diamanti, Olga and Mitliagkas, Ioannis and Guibas, Leonidas},
eprint = {1707.02392},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Achlioptas et al. - 2017 - Learning Representations and Generative Models for 3D Point Clouds(2).pdf:pdf},
month = {jul},
title = {{Learning Representations and Generative Models for 3D Point Clouds}},
url = {https://arxiv.org/abs/1707.02392 http://arxiv.org/abs/1707.02392},
year = {2017}
}
@inproceedings{Johnson2018,
abstract = {In theory, importance sampling speeds up stochastic gradient algorithms for supervised learning by prioritizing training examples. In practice, the cost of computing importances greatly limits the impact of importance sampling. We propose a robust, approximate importance sampling procedure (RAIS) for stochastic gradient descent. By approximating the ideal sampling distribution using robust optimization, RAIS provides much of the benefit of exact importance sampling with drastically reduced overhead. Empirically, we find RAIS-SGD and standard SGD follow similar learning curves, but RAIS moves faster through these paths, achieving speed-ups of at least 20{\%} and sometimes much more.},
author = {Johnson, Tyler B and Guestrin, Carlos},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Johnson, Guestrin - 2018 - Training Deep Models Faster with Robust, Approximate Importance Sampling.pdf:pdf},
issn = {10495258},
pages = {7265--7275},
title = {{Training deep models faster with robust, approximate importance sampling}},
volume = {2018-Decem},
year = {2018}
}
@article{Canziani2016,
abstract = {Since the emergence of Deep Neural Networks (DNNs) as a prominent technique in the field of computer vision, the ImageNet classification challenge has played a major role in advancing the state-of-the-art. While accuracy figures have steadily increased, the resource utilisation of winning models has not been properly taken into account. In this work, we present a comprehensive analysis of important metrics in practical applications: accuracy, memory footprint, parameters, operations count, inference time and power consumption. Key findings are: (1) power consumption is independent of batch size and architecture; (2) accuracy and inference time are in a hyperbolic relationship; (3) energy constraint is an upper bound on the maximum achievable accuracy and model complexity; (4) the number of operations is a reliable estimate of the inference time. We believe our analysis provides a compelling set of information that helps design and engineer efficient DNNs.},
archivePrefix = {arXiv},
arxivId = {1605.07678},
author = {Canziani, Alfredo and Paszke, Adam and Culurciello, Eugenio},
eprint = {1605.07678},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Canziani, Paszke, Culurciello - 2016 - An Analysis of Deep Neural Network Models for Practical Applications.pdf:pdf},
month = {may},
title = {{An Analysis of Deep Neural Network Models for Practical Applications}},
url = {http://arxiv.org/abs/1605.07678},
year = {2016}
}
@inproceedings{Shrestha2019,
abstract = {The accuracy of machine learning (ML) model is determined to a great extent by its training dataset. Yet the dataset optimization is often not the center of the focus to improve ML models. Datasets used in the training process can have a huge impact on the convergence of the training process and accuracy of the models. In this paper, we propose and implement importance sampling, a Monte Carlo method for variance reduction on training siamese networks to improve the accuracy of the image recognition. We demonstrate empirically that our approach can achieve improvement in training and testing errors on MNIST dataset compared to training when importance sampling is not used. Unlike standard convolution neural networks (CNN), siamese networks scale efficiently when the number of classes for image recognition increases. This paper is the first known attempt to combine importance sampling with siamese network and shows its effectiveness towards getting better accuracy.},
author = {Shrestha, Ajay and Mahmood, Ausif},
booktitle = {ICAART 2019 - Proceedings of the 11th International Conference on Agents and Artificial Intelligence},
doi = {10.5220/0007371706100615},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Shrestha, Mahmood - Unknown - Enhancing Siamese Networks Training with Importance Sampling.pdf:pdf},
isbn = {9789897583506},
keywords = {Convolution Neural Networks,Dataset Optimization,Importance Sampling,Siamese Networks},
pages = {610--615},
title = {{Enhancing Siamese networks training with importance sampling}},
volume = {2},
year = {2019}
}
@inproceedings{Albelwi2016,
abstract = {Deep Convolutional Neural networks (ConvNets) have achieved impressive results in several applications of computer vision and speech processing. With the availability of a large training set, it is common to find that the set contains useless samples (instances), either redundant or noisy. The process of removing these instances is called instance selection in the machine learning field. This paper evaluates the effectiveness of reducing the number of instances in the training set on deep (ConvNets) using a variety of instance selection methods to speed up the training time. We then study how these methods impact on classification accuracy. Moreover, many instance selection methods require a long running time for obtaining a representative subset of the dataset, especially if the dataset is large with high dimensionality. One of the popular algorithms in instance selection is Random Mutation Hill Climbing (RMHC). We propose a new approach in order to make RMHC work much faster with the same accuracy compared to original RMHC.},
author = {Albelwi, Saleh and Mahmood, Ausif},
booktitle = {2016 IEEE Long Island Systems, Applications and Technology Conference, LISAT 2016},
doi = {10.1109/LISAT.2016.7494142},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Albelwi, Mahmood - 2016 - Analysis of instance selection algorithms on large datasets with Deep Convolutional Neural Networks.pdf:pdf},
isbn = {9781467384902},
keywords = {ConvNets,Instance selection,Neasrest Neighbor classifier,Random mutation hill climbing},
month = {jun},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Analysis of instance selection algorithms on large datasets with Deep Convolutional Neural Networks}},
year = {2016}
}
@techreport{Mikolov,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Mikolov et al. - Unknown - Distributed Representations of Words and Phrases and their Compositionality.pdf:pdf},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
url = {https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf}
}
@article{Ozaki2019,
abstract = {The Google-Landmarks-v2 dataset is the biggest worldwide landmarks dataset characterized by a large magnitude of noisiness and diversity. We present a novel landmark retrieval/recognition system, robust to a noisy and diverse dataset, by our team, smlyaka. Our approach is based on deep convolutional neural networks with metric learning, trained by cosine-softmax based losses. Deep metric learning methods are usually sensitive to noise, and it could hinder to learn a reliable metric. To address this issue, we develop an automated data cleaning system. Besides, we devise a discriminative re-ranking method to address the diversity of the dataset for landmark retrieval. Using our methods, we achieved 1st place in the Google Landmark Retrieval 2019 challenge and 3rd place in the Google Landmark Recognition 2019 challenge on Kaggle.},
archivePrefix = {arXiv},
arxivId = {1906.04087},
author = {Ozaki, Kohei and Yokoo, Shuhei},
eprint = {1906.04087},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Ozaki, Yokoo - 2019 - Large-scale Landmark RetrievalRecognition under a Noisy and Diverse Dataset.pdf:pdf},
month = {jun},
title = {{Large-scale Landmark Retrieval/Recognition under a Noisy and Diverse Dataset}},
url = {http://arxiv.org/abs/1906.04087},
year = {2019}
}
@article{Istrate2019,
abstract = {In recent years an increasing number of researchers and practitioners have been suggesting algorithms for large-scale neural network architecture search: genetic algorithms, reinforcement learning, learning curve extrapolation, and accuracy predictors. None of them, however, demonstrated highperformance without training new experiments in the presence of unseen datasets. We propose a new deep neural network accuracy predictor, that estimates in fractions of a second classification performance for unseen input datasets, without training. In contrast to previously proposed approaches, our prediction is not only calibrated on the topological network information, but also on the characterization of the dataset-difficulty which allows us to re-tune the prediction without any training. Our predictor achieves a performance which exceeds 100 networks per second on a single GPU, thus creating the opportunity to perform large-scale architecture search within a few minutes. We present results of two searches performed in 400 seconds on a single GPU. Our best discovered networks reach 93.67{\%} accuracy for CIFAR-10 and 81.01{\%} for CIFAR-100, verified by training. These networks are performance competitive with other automatically discovered state-of-the-art networks however we only needed a small fraction of the time to solution and computational resources.},
archivePrefix = {arXiv},
arxivId = {1806.00250},
author = {Istrate, R. and Scheidegger, F. and Mariani, G. and Nikolopoulos, D. and Bekas, C. and Malossi, A. C. I.},
doi = {10.1609/aaai.v33i01.33013927},
eprint = {1806.00250},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Istrate et al. - 2018 - TAPAS Train-less Accuracy Predictor for Architecture Search.pdf:pdf},
issn = {2159-5399},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
month = {jun},
pages = {3927--3934},
publisher = {Association for the Advancement of Artificial Intelligence (AAAI)},
title = {{TAPAS: Train-Less Accuracy Predictor for Architecture Search}},
url = {http://arxiv.org/abs/1806.00250},
volume = {33},
year = {2019}
}
@inproceedings{Sun2014,
abstract = {Training Artificial Neural Networks (ANN) is relatively slow compared to many other machine learning algorithms. In this study, we focus on instance selection to improve training speed. We first evaluate the effectiveness of instance selection algorithms for k-nearest neighbor algorithms with ANN. We then analyze factors in accuracy-distance from decision boundary, dense regions, and class distributions, and propose new instance selection algorithms. We discuss the trade off between accuracy and training speed, and introduce a measure for the trade off. Our empirical results on real data sets indicate that our proposed RDI is more effective with ANN.},
author = {Sun, Xunhu and Chan, Philip K.},
booktitle = {Proceedings - 2014 13th International Conference on Machine Learning and Applications, ICMLA 2014},
doi = {10.1109/ICMLA.2014.52},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Sun, Chan - 2014 - An analysis of instance selection for neural networks to improve training speed.pdf:pdf},
isbn = {9781479974153},
keywords = {instance selection,neural networks,training speed},
month = {feb},
pages = {288--293},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{An analysis of instance selection for neural networks to improve training speed}},
year = {2014}
}
@techreport{Lorensen1987,
abstract = {We present a new algorithm, called marching cubes, that creates triangle models of constant density surfaces from 3D medical data. Using a divide-and-conquer approach to generate inter-slice connectivity, we create a case table that defines triangle topology. The algorithm processes the 3D medical data in scan-line order and calculates triangle vertices using linear interpolation. We find the gradient of the original data, normalize it, and use it as a basis for shading the models. The detail in images produced from the generated surface models is the result of maintaining the inter-slice connectivity, surface data, and gradient information present in the original 3D data. Results from computed tomography (CT), magnetic resonance (MR), and single-photon emission computed tomography (SPECT) illustrate the quality and functionality of marching cubes. We also discuss improvements that decrease processing time and add solid modeling capabilities.},
author = {Lorensen, William E and Cline, Harvey E},
booktitle = {Computer Graphics},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Lorensen, Cline - 1987 - MARCHING CUBES A HIGH RESOLUTION 3D SURFACE CONSTRUCTION ALGORITHM.pdf:pdf;:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Lorensen, Cline - 1987 - MARCHING CUBES A HIGH RESOLUTION 3D SURFACE CONSTRUCTION ALGORITHM(2).pdf:pdf},
keywords = {35 Additional Keywords: computer graphics,CR Categories: 33,medical imaging,surface reconstruction},
number = {4},
title = {{MARCHING CUBES: A HIGH RESOLUTION 3D SURFACE CONSTRUCTION ALGORITHM}},
url = {https://www2.cs.sfu.ca/{~}haoz/teaching/cmpt464/references/87{\_}Lorensen{\_}MarchingCubes.pdf},
volume = {21},
year = {1987}
}
@inproceedings{Zhao2015,
abstract = {Uniform sampling of training data has been commonly used in traditional stochastic optimization algorithms such as Proximal Stochastic Mirror Descent (prox-SMD) and Proximal Stochastic D-ual Coordinate Ascent (prox-SDCA). Although uniform sampling can guarantee that the sampled stochastic quantity is an unbiased estimate of the corresponding true quantity, the resulting estimator may have a rather high variance, which negatively affects the convergence of the underlying optimization procedure. In this paper we study s-tochastic optimization, including prox-SMD and prox-SDCA, with importance sampling, which improves the convergence rate by reducing the stochastic variance. We theoretically analyze the algorithms and empirically validate their effectiveness.},
author = {Zhao, Peilin and Zhang, Tong},
booktitle = {32nd International Conference on Machine Learning, ICML 2015},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Zhao, Zhang - Unknown - Stochastic Optimization with Importance Sampling for Regularized Loss Minimization.pdf:pdf},
isbn = {9781510810587},
pages = {1--9},
title = {{Stochastic optimization with importance sampling for regularized loss minimization}},
volume = {1},
year = {2015}
}
@article{Kingma2013,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
archivePrefix = {arXiv},
arxivId = {1312.6114},
author = {Kingma, Diederik P and Welling, Max},
eprint = {1312.6114},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Kingma, Welling - Unknown - Auto-Encoding Variational Bayes.pdf:pdf},
title = {{Auto-Encoding Variational Bayes}},
url = {https://arxiv.org/pdf/1312.6114.pdf http://arxiv.org/abs/1312.6114},
year = {2013}
}
@inproceedings{En2018,
abstract = {Learning-based image hashing consists in turning high-dimensional image features into compact binary codes, while preserving their semantic similarity (i.e., if two images are close in terms of content, their codes should be close as well). In this context, many existing hashing techniques rely on supervision for preserving these semantic properties. In this paper, we aim at learning such binary codes by exploiting the underlying structure of unlabeled data, using deep learning. The proposed deep network is based on a stacked convolutional autoencoder which hierarchically maps input images into a low-dimensional space. A binary relaxation constraint applied to the middle layer of the network - the one containing the code - makes the codes sparse and binary. To demonstrate the competitiveness of the proposed architecture, we evaluate the so produced hash codes on image retrieval and image classification tasks on the MNIST dataset, and compare its performance with state-of-the-art approaches.},
author = {En, Sovann and Cremilleux, Bruno and Jurie, Frederic},
booktitle = {Proceedings - International Conference on Image Processing, ICIP},
doi = {10.1109/ICIP.2017.8296917},
isbn = {9781509021758},
issn = {15224880},
keywords = {Convolutional autoencoder,Learning based hashing,Unsupervised learning},
month = {feb},
pages = {3420--3424},
publisher = {IEEE Computer Society},
title = {{Unsupervised deep hashing with stacked convolutional autoencoders}},
volume = {2017-Septe},
year = {2018}
}
@article{Bengio2013,
abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.},
author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
doi = {10.1109/TPAMI.2013.50},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Representation Learning- A Review and New Perspectives.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
number = {8},
pages = {1798--828},
pmid = {23787338},
publisher = {IEEE},
title = {{Representation learning: a review and new perspectives.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23787338},
volume = {35},
year = {2013}
}
@article{Mescheder2018,
abstract = {With the advent of deep neural networks, learning-based approaches for 3D reconstruction have gained popularity. However, unlike for images, in 3D there is no canonical representation which is both computationally and memory efficient yet allows for representing high-resolution geometry of arbitrary topology. Many of the state-of-the-art learning-based 3D reconstruction approaches can hence only represent very coarse 3D geometry or are limited to a restricted domain. In this paper, we propose Occupancy Networks, a new representation for learning-based 3D reconstruction methods. Occupancy networks implicitly represent the 3D surface as the continuous decision boundary of a deep neural network classifier. In contrast to existing approaches, our representation encodes a description of the 3D output at infinite resolution without excessive memory footprint. We validate that our representation can efficiently encode 3D structure and can be inferred from various kinds of input. Our experiments demonstrate competitive results, both qualitatively and quantitatively, for the challenging tasks of 3D reconstruction from single images, noisy point clouds and coarse discrete voxel grids. We believe that occupancy networks will become a useful tool in a wide variety of learning-based 3D tasks.},
archivePrefix = {arXiv},
arxivId = {1812.03828},
author = {Mescheder, Lars and Oechsle, Michael and Niemeyer, Michael and Nowozin, Sebastian and Geiger, Andreas},
eprint = {1812.03828},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Mescheder et al. - 2018 - Occupancy Networks Learning 3D Reconstruction in Function Space.pdf:pdf},
title = {{Occupancy Networks: Learning 3D Reconstruction in Function Space}},
url = {https://arxiv.org/pdf/1812.03828.pdf http://arxiv.org/abs/1812.03828},
year = {2018}
}
@article{Doersch2016,
abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
archivePrefix = {arXiv},
arxivId = {1606.05908},
author = {Doersch, Carl},
eprint = {1606.05908},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Doersch - 2016 - Tutorial on Variational Autoencoders.pdf:pdf},
keywords = {neural networks,structured prediction,unsupervised learning,variational autoencoders},
title = {{Tutorial on Variational Autoencoders}},
url = {https://arxiv.org/pdf/1606.05908.pdf http://arxiv.org/abs/1606.05908},
year = {2016}
}
@article{Scheidegger2018,
abstract = {In the deep-learning community new algorithms are published at an incredible pace. Therefore, solving an image classification problem for new datasets becomes a challenging task, as it requires to re-evaluate published algorithms and their different configurations in order to find a close to optimal classifier. To facilitate this process, before biasing our decision towards a class of neural networks or running an expensive search over the network space, we propose to estimate the classification difficulty of the dataset. Our method computes a single number that characterizes the dataset difficulty 27x faster than training state-of-the-art networks. The proposed method can be used in combination with network topology and hyper-parameter search optimizers to efficiently drive the search towards promising neural-network configurations.},
archivePrefix = {arXiv},
arxivId = {1803.09588},
author = {Scheidegger, Florian and Istrate, Roxana and Mariani, Giovanni and Benini, Luca and Bekas, Costas and Malossi, Cristiano},
eprint = {1803.09588},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Scheidegger et al. - 2018 - Efficient Image Dataset Classification Difficulty Estimation for Predicting Deep-Learning Accuracy.pdf:pdf},
month = {mar},
title = {{Efficient Image Dataset Classification Difficulty Estimation for Predicting Deep-Learning Accuracy}},
url = {http://arxiv.org/abs/1803.09588},
year = {2018}
}
@inproceedings{Nilsback2008,
abstract = {We investigate to what extent combinations of features can improve classification performance on a large dataset of similar classes. To this end we introduce a 103 class flower dataset. We compute four different features for the flowers, each describing different aspects, namely the local shape/texture, the shape of the boundary, the overall spatial distribution of petals, and the colour. We combine the features using a multiple kernel framework with a SVM classifier. The weights for each class are learnt using the method of Varma and Ray [16], which has achieved state of the art performance on other large dataset, such as Caltech 101/256. Our dataset has a similar challenge in the number of classes, but with the added difficulty of large between class similarity and small within class similarity. Results show that learning the optimum kernel combination of multiple features vastly improves the performance, from 55.1{\%} for the best single feature to 72.8{\%} for the combination of all features. {\textcopyright} 2008 IEEE.},
author = {Nilsback, Maria Elena and Zisserman, Andrew},
booktitle = {Proceedings - 6th Indian Conference on Computer Vision, Graphics and Image Processing, ICVGIP 2008},
doi = {10.1109/ICVGIP.2008.47},
isbn = {9780769534763},
pages = {722--729},
title = {{Automated flower classification over a large number of classes}},
year = {2008}
}
@incollection{Clarke1994,
abstract = {The Message Passing Interface Forum (MPIF), with participation from over 40 organizations, has been meeting since November 1992 to discuss and define a set of library standards for message passing. MPIF is not sanctioned or supported by any official standards organization. The goal of the Message Passing Interface, simply stated, is to develop a widely used standard for writing message-passing programs. As such the interface should establish a practical, portable, efficient and flexible standard for message passing. , This is the final report, Version 1.0, of the Message Passing Interface Forum. This document contains all the technical features proposed for the interface. This copy of the draft was processed by LATEX on April 21, 1994. , Please send comments on MPI to mpi-comments@cs.utk.edu. Your comment will be forwarded to MPIF committee members who will attempt to respond.},
author = {Clarke, Lyndon and Glendinning, Ian and Hempel, Rolf},
booktitle = {Programming Environments for Massively Parallel Distributed Systems},
doi = {10.1007/978-3-0348-8534-8_21},
pages = {213--218},
publisher = {Birkh{\"{a}}user Basel},
title = {{The MPI Message Passing Interface Standard}},
year = {1994}
}
@inproceedings{Shrivastava2016,
abstract = {The field of object detection has made significant advances riding on the wave of region-based ConvNets, but their training procedure still includes many heuristics and hyperparameters that are costly to tune. We present a simple yet surprisingly effective online hard example mining (OHEM) algorithm for training region-based ConvNet detectors. Our motivation is the same as it has always been - detection datasets contain an overwhelming number of easy examples and a small number of hard examples. Automatic selection of these hard examples can make training more effective and efficient. OHEM is a simple and intuitive algorithm that eliminates several heuristics and hyperparameters in common use. But more importantly, it yields consistent and significant boosts in detection performance on benchmarks like PASCAL VOC 2007 and 2012. Its effectiveness increases as datasets become larger and more difficult, as demonstrated by the results on the MS COCO dataset. Moreover, combined with complementary advances in the field, OHEM leads to state-of-the-art results of 78.9{\%} and 76.3{\%} mAP on PASCAL VOC 2007 and 2012 respectively.},
archivePrefix = {arXiv},
arxivId = {1604.03540},
author = {Shrivastava, Abhinav and Gupta, Abhinav and Girshick, Ross},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2016.89},
eprint = {1604.03540},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Shrivastava, Gupta, Girshick - 2016 - Training Region-based Object Detectors with Online Hard Example Mining.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
pages = {761--769},
title = {{Training region-based object detectors with online hard example mining}},
volume = {2016-Decem},
year = {2016}
}
@misc{CIrrus,
title = {{Cirrus, https://www.cirrus.ac.uk/}},
url = {https://www.cirrus.ac.uk/},
urldate = {2020-04-01}
}
@article{Russakovsky2015,
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
archivePrefix = {arXiv},
arxivId = {1409.0575},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
eprint = {1409.0575},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
month = {dec},
number = {3},
pages = {211--252},
publisher = {Springer New York LLC},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
volume = {115},
year = {2015}
}
@misc{convflops,
title = {{albanie/convnet-burden: Memory consumption and FLOP count estimates for convnets}},
url = {https://github.com/albanie/convnet-burden},
urldate = {2020-03-25}
}
@techreport{Wang,
abstract = {Both convolutional and recurrent operations are building blocks that process one local neighborhood at a time. In this paper, we present non-local operations as a generic family of building blocks for capturing long-range dependencies. Inspired by the classical non-local means method [4] in computer vision, our non-local operation computes the response at a position as a weighted sum of the features at all positions. This building block can be plugged into many computer vision architectures. On the task of video classification, even without any bells and whistles, our non-local models can compete or outperform current competition winners on both Kinetics and Charades datasets. In static image recognition, our non-local models improve object de-tection/segmentation and pose estimation on the COCO suite of tasks. Code will be made available.},
author = {Wang, Xiaolong and Girshick, Ross and Gupta, Abhinav and He, Kaiming},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - Unknown - Non-local Neural Networks.pdf:pdf},
title = {{Non-local Neural Networks}},
url = {http://openaccess.thecvf.com/content{\_}cvpr{\_}2018/papers/Wang{\_}Non-Local{\_}Neural{\_}Networks{\_}CVPR{\_}2018{\_}paper.pdf}
}
@inproceedings{Choy2016,
abstract = {Inspired by the recent success of methods that employ shape priors to achieve robust 3D reconstructions, we propose a novel recurrent neural network architecture that we call the 3D Recurrent Reconstruction Neural Network (3D-R2N2). The network learns a mapping from images of objects to their underlying 3D shapes from a large collection of synthetic data. Our network takes in one or more images of an object instance from arbitrary viewpoints and outputs a reconstruction of the object in the form of a 3D occupancy grid. Unlike most of the previous works, our network does not require any image annotations or object class labels for training or testing. Our extensive experimental analysis shows that our reconstruction framework i) outperforms the state-of-the-art methods for single view reconstruction, and ii) enables the 3D reconstruction of objects in situations when traditional SFM/SLAM methods fail (because of lack of texture and/or wide baseline).},
archivePrefix = {arXiv},
arxivId = {1604.00449},
author = {Choy, Christopher B. and Xu, Danfei and Gwak, Jun Young and Chen, Kevin and Savarese, Silvio},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-46484-8_38},
eprint = {1604.00449},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Choy et al. - 2016 - 3D-R2N2 A Unified Approach for Single and Multi-view 3D Object Reconstruction.pdf:pdf},
isbn = {9783319464831},
issn = {16113349},
keywords = {Multi-view,Reconstruction,Recurrent neural network},
month = {apr},
pages = {628--644},
title = {{3D-R2N2: A unified approach for single and multi-view 3D object reconstruction}},
url = {http://arxiv.org/abs/1604.00449},
volume = {9912 LNCS},
year = {2016}
}
@misc{Krizhevsky2009,
abstract = {Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it dicult to learn a good set of lters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels.},
author = {Krizhevsky, Alex},
pages = {1--58},
title = {{Learning multiple layers of features from tiny images. Tech. rep., CIFAR-10 (Canadian Institute for Advanced Research)}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.222.9220{\&}rep=rep1{\&}type=pdf http://www.cs.toronto.edu/{~}kriz/cifar.html},
urldate = {2020-08-01},
year = {2009}
}
@inproceedings{Simonyan2015,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
booktitle = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
eprint = {1409.1556},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Simonyan, Zisserman - 2015 - Very deep convolutional networks for large-scale image recognition.pdf:pdf},
month = {sep},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Very deep convolutional networks for large-scale image recognition}},
year = {2015}
}
@inproceedings{Chang2017,
abstract = {Self-paced learning and hard example mining re-weight training instances to improve learning accuracy. This paper presents two improved alternatives based on lightweight estimates of sample uncertainty in stochastic gradient descent (SGD): the variance in predicted probability of the correct class across iterations of mini-batch SGD, and the proximity of the correct class probability to the decision threshold. Extensive experimental results on six datasets show that our methods reliably improve accuracy in various network architectures, including additional gains on top of other popular training techniques, such as residual learning, momentum, ADAM, batch normalization, dropout, and distillation.},
archivePrefix = {arXiv},
arxivId = {1704.07433},
author = {Chang, Haw Shiuan and Learned-Miller, Erik and McCallum, Andrew},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1704.07433},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Chang, Learned-Miller, Mccallum - 2017 - Active Bias Training More Accurate Neural Networks by Emphasizing High Variance Samples.pdf:pdf},
issn = {10495258},
pages = {1003--1013},
title = {{Active bias: Training more accurate neural networks by emphasizing high variance samples}},
volume = {2017-Decem},
year = {2017}
}
@article{YiyiLiao2018,
abstract = {Existing learning based solutions to 3D surface predic-tion cannot be trained end-to-end as they operate on inter-mediate representations (e.g., TSDF) from which 3D sur-face meshes must be extracted in a post-processing step (e.g., via the marching cubes algorithm). In this paper, we investigate the problem of end-to-end 3D surface predic-tion. We first demonstrate that the marching cubes algo-rithm is not differentiable and propose an alternative differ-entiable formulation which we insert as a final layer into a 3D convolutional neural network. We further propose a set of loss functions which allow for training our model with sparse point supervision. Our experiments demon-strate that the model allows for predicting sub-voxel accu-rate 3D shapes of arbitrary topology. Additionally, it learns to complete shapes and to separate an object's inside from its outside even in the presence of sparse and incomplete ground truth. We investigate the benefits of our approach on the task of inferring shapes from 3D point clouds. Our model is flexible and can be combined with a variety of shape encoder and shape inference techniques.},
author = {{Yiyi Liao} and Donn´, Simon and Geiger, Andreas},
doi = {10.1109/CVPR.2018.00308},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Liao, Donn{\'{e}}, Geiger - 2018 - Deep Marching Cubes Learning Explicit Surface Representations.pdf:pdf;:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Yiyi Liao, Donn´, Geiger - 2018 - Deep Marching Cubes Learning Explicit Surface Representations.pdf:pdf},
number = {April},
pages = {23--25},
title = {{Deep Marching Cubes: Learning Explicit Surface Representations}},
url = {http://openaccess.thecvf.com/content{\_}cvpr{\_}2018/html/Liao{\_}Deep{\_}Marching{\_}Cubes{\_}CVPR{\_}2018{\_}paper.html},
year = {2018}
}
@article{Dagum1998,
abstract = {At its most elemental level, OpenMP is a set of compiler directives and callable runtime library routines that extend Fortran (and separately, C and C++ to express shared memory parallelism. It leaves the base language unspecified, and vendors can implement OpenMP in any Fortran compiler. Naturally, to support pointers and allocatables, Fortran 90 and Fortran 95 require the OpenMP implementation to include additional semantics over Fortran 77. OpenMP leverages many of the X3H5 concepts while extending them to support coarse grain parallelism. The standard also includes a callable runtime library with accompanying environment variables},
author = {Dagum, L. and Menon, R.},
doi = {10.1109/99.660313},
issn = {10709924},
journal = {IEEE Computational Science and Engineering},
number = {1},
pages = {46--55},
publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
title = {{OpenMP: an industry standard API for shared-memory programming}},
url = {http://ieeexplore.ieee.org/document/660313/},
volume = {5},
year = {1998}
}
@inproceedings{Zoph2018,
abstract = {Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (which we call the 'NASNet search space') which enables transferability. In our experiments, we search for the best convolutional layer (or 'cell') on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, which we name a 'NASNet architecture'. We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, a NASNet found by our method achieves 2.4{\%} error rate, which is state-of-the-art. Although the cell is not searched for directly on ImageNet, a NASNet constructed from the best cell achieves, among the published works, state-of-the-art accuracy of 82.7{\%} top-1 and 96.2{\%} top-5 on ImageNet. Our model is 1.2{\%} better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28{\%} in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74{\%} top-1 accuracy, which is 3.1{\%} better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the image features learned from image classification are generically useful and can be transferred to other computer vision problems. On the task of object detection, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0{\%} achieving 43.1{\%} mAP on the COCO dataset.},
archivePrefix = {arXiv},
arxivId = {1707.07012},
author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00907},
eprint = {1707.07012},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Zoph et al. - 2018 - Learning Transferable Architectures for Scalable Image Recognition.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
month = {jul},
pages = {8697--8710},
publisher = {IEEE Computer Society},
title = {{Learning Transferable Architectures for Scalable Image Recognition}},
url = {http://arxiv.org/abs/1707.07012},
year = {2018}
}
@inproceedings{Stutz2018,
abstract = {3D shape completion from partial point clouds is a fun-damental problem in computer vision and computer graph-ics. Recent approaches can be characterized as either data-driven or learning-based. Data-driven approaches rely on a shape model whose parameters are optimized to fit the ob-servations. Learning-based approaches, in contrast, avoid the expensive optimization step and instead directly pre-dict the complete shape from the incomplete observations using deep neural networks. However, full supervision is required which is often not available in practice. In this work, we propose a weakly-supervised learning-based ap-proach to 3D shape completion which neither requires slow optimization nor direct supervision. While we also learn a shape prior on synthetic data, we amortize, i.e., learn, maxi-mum likelihood fitting using deep neural networks resulting in efficient shape completion without sacrificing accuracy. Tackling 3D shape completion of cars on ShapeNet [5] and KITTI [18], we demonstrate that the proposed amortized maximum likelihood approach is able to compete with a fully supervised baseline and a state-of-the-art data-driven approach while being significantly faster. On ModelNet [49], we additionally show that the approach is able to gen-eralize to other object categories as well.},
author = {Stutz, David and Geiger, Andreas},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00209},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Stutz, Geiger - 2018 - Learning 3D Shape Completion from Laser Scan Data with Weak Supervision(2).pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
pages = {1955--1964},
title = {{Learning 3D Shape Completion from Laser Scan Data with Weak Supervision}},
url = {http://openaccess.thecvf.com/content{\_}cvpr{\_}2018/html/Stutz{\_}Learning{\_}3D{\_}Shape{\_}CVPR{\_}2018{\_}paper.html},
year = {2018}
}
@inproceedings{Tatarchenko2017,
abstract = {We present a deep convolutional decoder architecture that can generate volumetric 3D outputs in a compute- and memory-efficient manner by using an octree representation. The network learns to predict both the structure of the octree, and the occupancy values of individual cells. This makes it a particularly valuable technique for generating 3D shapes. In contrast to standard decoders acting on regular voxel grids, the architecture does not have cubic complexity. This allows representing much higher resolution outputs with a limited memory budget. We demonstrate this in several application domains, including 3D convolutional autoencoders, generation of objects and whole scenes from high-level representations, and shape from a single image.},
author = {Tatarchenko, Maxim and Dosovitskiy, Alexey and Brox, Thomas},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2017.230},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Tatarchenko, Dosovitskiy, Brox - 2017 - Octree Generating Networks Efficient Convolutional Architectures for High-Resolution 3D Outputs.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
pages = {2107--2115},
title = {{Octree Generating Networks: Efficient Convolutional Architectures for High-resolution 3D Outputs}},
url = {http://openaccess.thecvf.com/content{\_}iccv{\_}2017/html/Tatarchenko{\_}Octree{\_}Generating{\_}Networks{\_}ICCV{\_}2017{\_}paper.html},
volume = {2017-Octob},
year = {2017}
}
@inproceedings{Kornblith2018,
abstract = {Transfer learning is a cornerstone of computer vision, yet little work has been done to evaluate the relationship between architecture and transfer. An implicit hypothesis in modern computer vision research is that models that perform better on ImageNet necessarily perform better on other vision tasks. However, this hypothesis has never been systematically tested. Here, we compare the performance of 16 classification networks on 12 image classification datasets. We find that, when networks are used as fixed feature extractors or fine-tuned, there is a strong correlation between ImageNet accuracy and transfer accuracy (r = 0.99 and 0.96, respectively). In the former setting, we find that this relationship is very sensitive to the way in which networks are trained on ImageNet; many common forms of regularization slightly improve ImageNet accuracy but yield features that are much worse for transfer learning. Additionally, we find that, on two small fine-grained image classification datasets, pretraining on ImageNet provides minimal benefits, indicating the learned features from ImageNet do not transfer well to fine-grained tasks. Together, our results show that ImageNet architectures generalize well across datasets, but ImageNet features are less general than previously suggested.},
archivePrefix = {arXiv},
arxivId = {1805.08974},
author = {Kornblith, Simon and Shlens, Jonathon and Le, Quoc V.},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2019.00277},
eprint = {1805.08974},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Kornblith, Shlens, Le - 2018 - Do Better ImageNet Models Transfer Better.pdf:pdf},
isbn = {9781728132938},
issn = {10636919},
keywords = {Deep Learning,Representation Learning},
month = {may},
pages = {2656--2666},
title = {{Do better imagenet models transfer better?}},
volume = {2019-June},
year = {2019}
}
@inproceedings{Hacohen2019,
abstract = {Training neural networks is traditionally done by providing a sequence of random mini-batches sampled uniformly from the entire training data. In this work, we analyze the effect of curriculum learning, which involves the non-uniform sampling of mini-batches, on the training of deep networks, and specifically CNNs trained for image recognition. To employ curriculum learning, the training algorithm must resolve 2 problems: (i) sort the training examples by difficulty; (ii) compute a series of mini-batches that exhibit an increasing level of difficulty. We address challenge (i) using two methods: transfer learning from some competitive "teacher" network, and bootstrapping. In our empirical evaluation, both methods show similar benefits in terms of increased learning speed and improved final performance on test data. We address challenge (ii) by investigating different pacing functions to guide the sampling. The empirical investigation includes a variety of network architectures, using images from CIFAR-10, CIFAR-100 and subsets of ImageNet. We conclude with a novel theoretical analysis of curriculum learning, where we show how it effectively modifies the optimization landscape. We then define the concept of an ideal curriculum, and show that under mild conditions it does not change the corresponding global minimum of the optimization function.},
archivePrefix = {arXiv},
arxivId = {1904.03626},
author = {Hacohen, Guy and Weinshall, Daphna},
booktitle = {36th International Conference on Machine Learning, ICML 2019},
eprint = {1904.03626},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Hacohen, Weinshall - 2019 - On The Power of Curriculum Learning in Training Deep Networks.pdf:pdf},
isbn = {9781510886988},
month = {apr},
pages = {4483--4496},
publisher = {International Machine Learning Society (IMLS)},
title = {{On the power of curriculum learning in training deep networks}},
url = {http://arxiv.org/abs/1904.03626},
volume = {2019-June},
year = {2019}
}
@article{Loshchilov2015,
abstract = {Deep neural networks are commonly trained using stochastic non-convex optimization procedures, which are driven by gradient information estimated on fractions (batches) of the dataset. While it is commonly accepted that batch size is an important parameter for offline tuning, the benefits of online selection of batches remain poorly understood. We investigate online batch selection strategies for two state-of-the-art methods of stochastic gradient-based optimization, AdaDelta and Adam. As the loss function to be minimized for the whole dataset is an aggregation of loss functions of individual datapoints, intuitively, datapoints with the greatest loss should be considered (selected in a batch) more frequently. However, the limitations of this intuition and the proper control of the selection pressure over time are open questions. We propose a simple strategy where all datapoints are ranked w.r.t. their latest known loss value and the probability to be selected decays exponentially as a function of rank. Our experimental results on the MNIST dataset suggest that selecting batches speeds up both AdaDelta and Adam by a factor of about 5.},
archivePrefix = {arXiv},
arxivId = {1511.06343},
author = {Loshchilov, Ilya and Hutter, Frank},
eprint = {1511.06343},
month = {nov},
title = {{Online Batch Selection for Faster Training of Neural Networks}},
url = {https://arxiv.org/abs/1511.06343v4 http://arxiv.org/abs/1511.06343},
year = {2015}
}
@techreport{Bengio2003,
abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
author = {Bengio, Yoshua and Ducharme, R{\'{e}}jean and Vincent, Pascal and Jauvin, Christian and Ca, Jauvinc@iro Umontreal and Kandola, Jaz and Hofmann, Thomas and Poggio, Tomaso and Shawe-Taylor, John},
booktitle = {Journal of Machine Learning Research},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Bengio et al. - 2003 - A Neural Probabilistic Language Model.pdf:pdf},
keywords = {Statistical language modeling,artificial neural networks,curse of dimensionality,distributed representation},
pages = {1137--1155},
title = {{A Neural Probabilistic Language Model}},
url = {http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf},
volume = {3},
year = {2003}
}
@inproceedings{Groueix2018,
abstract = {We introduce a method for learning to generate the surface of 3D shapes. Our approach represents a 3D shape as a collection ofparametric surface elements and, in contrast to methods generating voxel grids or point clouds, naturally infers a surface representation ofthe shape. Beyond its novelty, our new shape generation framework, AtlasNet, comes with significant advantages, such as improved precision and generalization capabilities, and the possibility to generate a shape ofarbitrary resolution without memory issues. We demonstrate these benefits and compare to strong baselines on the ShapeNet benchmark for two applications: (i) auto-encoding shapes, and (ii) single-view reconstruction from a still image. We also provide results showing its potential for other applications, such as morphing, parametrization, super-resolution, matching, and co-segmentation.},
archivePrefix = {arXiv},
arxivId = {1802.05384v3},
author = {Groueix, Thibault and Fisher, Matthew and Kim, Vladimir G. and Russell, Bryan C. and Aubry, Mathieu},
booktitle = {Computer Vision and Pattern Recognition},
eprint = {1802.05384v3},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Groueix et al. - 2018 - AtlasNet A Papier-M{\^{a}}ch{\'{e}} Approach to Learning 3D Surface Generation(2).pdf:pdf},
pages = {216--224},
title = {{AtlasNet: A Papier-M{\^{a}}ch{\'{e}} Approach to Learning 3D Surface Generation}},
url = {http://openaccess.thecvf.com/content{\_}cvpr{\_}2018/html/Groueix{\_}A{\_}Papier-Mache{\_}Approach{\_}CVPR{\_}2018{\_}paper.html http://openaccess.thecvf.com/content{\_}cvpr{\_}2018/html/Groueix{\_}A{\_}Papier-Mache{\_}Approach{\_}CVPR{\_}2018{\_}paper.html{\%}250Ahttp://imagine.enpc.fr/{~}groueixt/atlasnet},
year = {2018}
}
@article{Yan2016,
abstract = {Understanding the 3D world is a fundamental problem in computer vision. However, learning a good representation of 3D objects is still an open problem due to the high dimensionality of the data and many factors of variation involved. In this work, we investigate the task of single-view 3D object reconstruction from a learning agent's perspective. We formulate the learning process as an interaction between 3D and 2D representations and propose an encoder-decoder network with a novel projection loss defined by the perspective transformation. More importantly, the projection loss enables the unsupervised learning using 2D observation without explicit 3D supervision. We demonstrate the ability of the model in generating 3D volume from a single 2D image with three sets of experiments: (1) learning from single-class objects; (2) learning from multi-class objects and (3) testing on novel object classes. Results show superior performance and better generalization ability for 3D object reconstruction when the projection loss is involved.},
archivePrefix = {arXiv},
arxivId = {1612.00814},
author = {Yan, Xinchen and Yang, Jimei and Yumer, Ersin and Guo, Yijie and Lee, Honglak},
eprint = {1612.00814},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Yan et al. - 2016 - Perspective Transformer Nets Learning Single-View 3D Object Reconstruction without 3D Supervision.pdf:pdf},
pages = {1696--1704},
title = {{Perspective Transformer Nets: Learning Single-View 3D Object Reconstruction without 3D Supervision}},
url = {http://papers.nips.cc/paper/6205-perspective-transformer-nets-learning-single-view-3d-object-reconstruction-without-3d-supervision http://arxiv.org/abs/1612.00814},
year = {2016}
}
@article{Katharopoulos2018,
abstract = {Deep neural network training spends most of the computation on examples that are properly handled, and could be ignored. We propose to mitigate this phenomenon with a principled importance sampling scheme that focuses computation on "informative" examples, and reduces the variance of the stochastic gradients during training. Our contribution is twofold: first, we derive a tractable upper bound to the per-sample gradient norm, and second we derive an estimator of the variance reduction achieved with importance sampling, which enables us to switch it on when it will result in an actual speedup. The resulting scheme can be used by changing a few lines of code in a standard SGD procedure, and we demonstrate experimentally, on image classification, CNN fine-tuning, and RNN training, that for a fixed wall-clock time budget, it provides a reduction of the train losses of up to an order of magnitude and a relative improvement of test errors between 5{\%} and 17{\%}.},
archivePrefix = {arXiv},
arxivId = {1803.00942},
author = {Katharopoulos, Angelos and Fleuret, Fran{\c{c}}ois},
eprint = {1803.00942},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Katharopoulos, Fleuret - 2018 - Not All Samples Are Created Equal Deep Learning with Importance Sampling.pdf:pdf},
journal = {35th International Conference on Machine Learning, ICML 2018},
month = {mar},
pages = {3936--3949},
publisher = {International Machine Learning Society (IMLS)},
title = {{Not All Samples Are Created Equal: Deep Learning with Importance Sampling}},
url = {http://arxiv.org/abs/1803.00942},
volume = {6},
year = {2018}
}
@article{Russakovsky2014,
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
archivePrefix = {arXiv},
arxivId = {1409.0575},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
eprint = {1409.0575},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
month = {sep},
number = {3},
pages = {211--252},
publisher = {Springer New York LLC},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
url = {http://arxiv.org/abs/1409.0575},
volume = {115},
year = {2015}
}
@article{Fulgeri2019a,
abstract = {When you see a person in a crowd, occluded by other persons, you miss visual information that can be used to recognize, re-identify or simply classify him or her. You can imagine its appearance given your experience, nothing more. Similarly, AI solutions can try to hallucinate missing information with specific deep learning architectures, suitably trained with people with and without occlusions. The goal of this work is to generate a complete image of a person, given an occluded version in input, that should be a) without occlusion b) similar at pixel level to a completely visible people shape c) capable to conserve similar visual attributes (e.g. male/female) of the original one. For the purpose, we propose a new approach by integrating the state-of-the-art of neural network architectures, namely U-nets and GANs, as well as discriminative attribute classification nets, with an architecture specifically designed to de-occlude people shapes. The network is trained to optimize a Loss function which could take into account the aforementioned objectives. As well we propose two datasets for testing our solution: the first one, occluded RAP, created automatically by occluding real shapes of the RAP dataset created by Li et al. (2016) (which collects also attributes of the people aspect); the second is a large synthetic dataset, AiC, generated in computer graphics with data extracted from the GTA video game, that contains 3D data of occluded objects by construction. Results are impressive and outperform any other previous proposal. This result could be an initial step to many further researches to recognize people and their behavior in an open crowded world.},
author = {Fulgeri, Federico and Fabbri, Matteo and Alletto, Stefano and Calderara, Simone and Cucchiara, Rita},
doi = {10.1016/J.CVIU.2019.03.007},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Fulgeri et al. - 2019 - Can adversarial networks hallucinate occluded people with a plausible aspect.pdf:pdf},
issn = {1077-3142},
journal = {Computer Vision and Image Understanding},
month = {may},
pages = {71--80},
publisher = {Academic Press},
title = {{Can adversarial networks hallucinate occluded people with a plausible aspect?}},
url = {https://www.sciencedirect.com/science/article/pii/S1077314219300438},
volume = {182},
year = {2019}
}
@article{Riquelme2003a,
abstract = {This paper presents a new approach to finding representative patterns for dataset editing. The algorithm patterns by ordered projections (POP), has some interesting characteristics: Important reduction of the number of instances from the dataset; lower computational cost ($\Theta$(mn log n)) with respect to other typical algorithms due to the absence of distance calculations; conservation of the decision boundaries, especially from the point of view of the application of axis-parallel classifiers. POP works well in practice with both continuous and discrete attributes. The performance of POP is analysed in two ways: Percentage of reduction and classification. POP has been compared to IB2, ENN and SHRINK concerning the percentage of reduction and the computational cost. In addition, we have analysed the accuracy of k-NN and C4.5 after applying the reduction techniques. An extensive empirical study using datasets with continuous and discrete attributes from the UCI repository shows that POP is a valuable preprocessing method for the later application of any axis-parallel learning algorithm. {\textcopyright} 2002 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.},
author = {Riquelme, Jos{\'{e}} C. and Aguilar-Ruiz, Jes{\'{u}}s S. and Toro, Miguel},
doi = {10.1016/S0031-3203(02)00119-X},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Riquelme, Aguilar-Ruiz, Toro - 2003 - Finding representative patterns with ordered projections(2).pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Axis-parallel classifiers,Data mining,Pattern analysis,Preprocessing techniques},
month = {apr},
number = {4},
pages = {1009--1018},
publisher = {Elsevier Ltd},
title = {{Finding representative patterns with ordered projections}},
volume = {36},
year = {2003}
}
@article{LeCun1998,
abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient-based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of two dimensional (2-D) shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN's), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank check is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day. {\textcopyright} 1998 IEEE.},
author = {LeCun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR)},
number = {11},
pages = {2278--2323},
title = {{Gradient-based learning applied to document recognition}},
volume = {86},
year = {1998}
}
@techreport{Pennington,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log-bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co-occurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful sub-structure, as evidenced by its performance of 75{\%} on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Pennington, Socher, Manning - Unknown - GloVe Global Vectors for Word Representation.pdf:pdf},
pages = {1532--1543},
title = {{GloVe: Global Vectors for Word Representation}},
url = {http://nlp.}
}
@inproceedings{He2016a,
abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR- 10 (4.62{\%} error) and CIFAR-100, and a 200-layer ResNet on ImageNet.},
archivePrefix = {arXiv},
arxivId = {1603.05027},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-46493-0_38},
eprint = {1603.05027},
isbn = {9783319464923},
issn = {16113349},
month = {mar},
pages = {630--645},
publisher = {Springer Verlag},
title = {{Identity mappings in deep residual networks}},
volume = {9908 LNCS},
year = {2016}
}
@inproceedings{Sun2017,
abstract = {The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10 × or 100 × ? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between 'enormous data' and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.},
archivePrefix = {arXiv},
arxivId = {1707.02968},
author = {Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2017.97},
eprint = {1707.02968},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Sun et al. - 2017 - Revisiting Unreasonable Effectiveness of Data in Deep Learning Era.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
pages = {843--852},
title = {{Revisiting Unreasonable Effectiveness of Data in Deep Learning Era}},
volume = {2017-Octob},
year = {2017}
}
@inproceedings{Bengio2009,
abstract = {Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them "curriculum learning". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions). Copyright 2009.},
address = {New York, New York, USA},
author = {Bengio, Yoshua and Louradour, J{\'{e}}r̂ome and Collobert, Ronan and Weston, Jason},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/1553374.1553380},
file = {:Users/hengyuwen/Library/Application Support/Mendeley Desktop/Downloaded/Bengio et al. - 2009 - Curriculum learning(3).pdf:pdf},
isbn = {9781605585161},
pages = {1--8},
publisher = {ACM Press},
title = {{Curriculum learning}},
url = {http://portal.acm.org/citation.cfm?doid=1553374.1553380},
volume = {382},
year = {2009}
}
